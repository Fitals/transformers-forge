{
    "architectures": [
        "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "hidden_size": 2048,
    "num_hidden_layers": 24,
    "vocab_size": 151936
}