"""
üî¨ EMA Benchmark ‚Äî –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ (v2)
====================================================

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç EMA –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏.

–ú–ï–¢–û–î–û–õ–û–ì–ò–Ø:
- –û–±—É—á–∞–µ–º –û–î–ù–£ –º–æ–¥–µ–ª—å —Å EMA
- –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º normal weights vs EMA weights –û–î–ù–û–ô –º–æ–¥–µ–ª–∏
- –≠—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é –ø–æ–ª—å–∑—É EMA

–ó–∞–ø—É—Å–∫:
    python benchmarks/ema_benchmark.py

–ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: 30-60 —Å–µ–∫—É–Ω–¥ –Ω–∞ CPU
"""

import time
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))


def run_benchmark():
    """–ó–∞–ø—É—Å–∫ benchmark —Å—Ä–∞–≤–Ω–µ–Ω–∏—è EMA vs Normal weights."""
    
    print("=" * 60)
    print("üî¨ EMA BENCHMARK v2 ‚Äî Transformers Forge")
    print("=" * 60)
    print()
    print("üìã –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø:")
    print("   –û–±—É—á–∞–µ–º –û–î–ù–£ –º–æ–¥–µ–ª—å —Å EMA –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º:")
    print("   - Normal weights (—Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è)")
    print("   - EMA weights (—É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–µ –≤–µ—Å–∞)")
    print()
    
    # –ò–º–ø–æ—Ä—Ç—ã
    print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫...")
    
    try:
        import torch
        import torch.nn as nn
        from torch.utils.data import DataLoader, TensorDataset
    except ImportError:
        print("‚ùå PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch")
        return
    
    try:
        from transformers.ema import create_ema_state, update_ema_state, apply_ema_state
    except ImportError:
        print("‚ùå Transformers Forge –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install -e .")
        return
    
    print("‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    print()
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    HIDDEN_SIZE = 256
    NUM_LAYERS = 4
    BATCH_SIZE = 32
    NUM_SAMPLES = 2000
    NUM_STEPS = 300
    LEARNING_RATE = 2e-3  # –í—ã—Å–æ–∫–∏–π LR –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —à—É–º–∞
    EMA_DECAY = 0.99
    
    print("‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"   Hidden size: {HIDDEN_SIZE}")
    print(f"   Layers: {NUM_LAYERS}")
    print(f"   Batch size: {BATCH_SIZE}")
    print(f"   Training steps: {NUM_STEPS}")
    print(f"   Learning rate: {LEARNING_RATE} (–≤—ã—Å–æ–∫–∏–π –¥–ª—è —à—É–º–∞)")
    print(f"   EMA decay: {EMA_DECAY}")
    print()
    
    # –°–æ–∑–¥–∞—ë–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å
    class SimpleTransformer(nn.Module):
        def __init__(self, hidden_size, num_layers):
            super().__init__()
            self.embedding = nn.Linear(hidden_size, hidden_size)
            self.layers = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(hidden_size, hidden_size * 4),
                    nn.GELU(),
                    nn.Linear(hidden_size * 4, hidden_size),
                    nn.LayerNorm(hidden_size)
                )
                for _ in range(num_layers)
            ])
            self.head = nn.Linear(hidden_size, hidden_size)
        
        def forward(self, x):
            x = self.embedding(x)
            for layer in self.layers:
                x = x + layer(x)
            return self.head(x)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å —à—É–º–æ–º
    print("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
    torch.manual_seed(42)
    X = torch.randn(NUM_SAMPLES, HIDDEN_SIZE)
    # Target —Å —à—É–º–æ–º (—Å–∏–º—É–ª—è—Ü–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
    Y = torch.sin(X) * 0.5 + torch.randn_like(X) * 0.2  # –ë–æ–ª—å—à–µ —à—É–º–∞
    
    dataset = TensorDataset(X, Y)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    
    # Evaluation set (—á–∏—Å—Ç—ã–π, –±–µ–∑ —à—É–º–∞ –≤ target)
    X_eval = torch.randn(500, HIDDEN_SIZE)
    Y_eval = torch.sin(X_eval) * 0.5  # –ë–µ–∑ —à—É–º–∞ ‚Äî –∏—Å—Ç–∏–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    
    def evaluate(model, X_eval, Y_eval):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏."""
        model.eval()
        with torch.no_grad():
            pred = model(X_eval)
            loss = nn.MSELoss()(pred, Y_eval)
        model.train()
        return loss.item()
    
    # ========================================================================
    # –û–±—É—á–µ–Ω–∏–µ —Å EMA
    # ========================================================================
    print()
    print("-" * 60)
    print("üü¢ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å EMA tracking")
    print("-" * 60)
    
    model = SimpleTransformer(HIDDEN_SIZE, NUM_LAYERS)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.MSELoss()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º EMA
    ema_state = create_ema_state(model)
    
    eval_normal_history = []
    eval_ema_history = []
    
    start_time = time.time()
    step = 0
    
    for epoch in range(20):
        for batch_x, batch_y in dataloader:
            optimizer.zero_grad()
            pred = model(batch_x)
            loss = criterion(pred, batch_y)
            loss.backward()
            optimizer.step()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º EMA
            update_ema_state(model, ema_state, decay=EMA_DECAY)
            
            if step % 30 == 0:
                # Eval —Å normal –≤–µ—Å–∞–º–∏
                eval_normal = evaluate(model, X_eval, Y_eval)
                eval_normal_history.append(eval_normal)
                
                # Eval —Å EMA –≤–µ—Å–∞–º–∏
                backup = apply_ema_state(model, ema_state)
                eval_ema = evaluate(model, X_eval, Y_eval)
                eval_ema_history.append(eval_ema)
                apply_ema_state(model, backup)
                
                diff = ((eval_normal - eval_ema) / eval_normal) * 100 if eval_normal > 0 else 0
                marker = "‚úÖ" if eval_ema < eval_normal else "‚ö†Ô∏è"
                
                print(f"   Step {step:3d} | Normal: {eval_normal:.4f} | EMA: {eval_ema:.4f} | {marker} {diff:+.1f}%")
            
            step += 1
            if step >= NUM_STEPS:
                break
        if step >= NUM_STEPS:
            break
    
    training_time = time.time() - start_time
    
    # ========================================================================
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    # ========================================================================
    print()
    print("-" * 60)
    print("üìä –§–ò–ù–ê–õ–¨–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï")
    print("-" * 60)
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–π eval —Å normal –≤–µ—Å–∞–º–∏
    final_normal = evaluate(model, X_eval, Y_eval)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º EMA –∏ eval
    backup = apply_ema_state(model, ema_state)
    final_ema = evaluate(model, X_eval, Y_eval)
    
    improvement = ((final_normal - final_ema) / final_normal) * 100
    
    print()
    print(f"   {'–ú–µ—Ç—Ä–∏–∫–∞':<25} {'Normal':<15} {'EMA':<15} {'–†–∞–∑–Ω–∏—Ü–∞':<15}")
    print(f"   {'-'*25} {'-'*15} {'-'*15} {'-'*15}")
    print(f"   {'Final Eval Loss':<25} {final_normal:<15.4f} {final_ema:<15.4f} {improvement:+.1f}%")
    print()
    
    if improvement > 0:
        print(f"   ‚úÖ EMA —É–ª—É—á—à–∏–ª –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ {improvement:.1f}%!")
        print(f"   üìå EMA –≤–µ—Å–∞ –ª—É—á—à–µ —á–µ–º normal –≤–µ—Å–∞ –û–î–ù–û–ô –º–æ–¥–µ–ª–∏")
    else:
        print(f"   ‚ö†Ô∏è EMA –Ω–µ –ø–æ–∫–∞–∑–∞–ª —É–ª—É—á—à–µ–Ω–∏—è ({improvement:.1f}%)")
    
    # ========================================================================
    # –ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
    # ========================================================================
    print()
    print("-" * 60)
    print("üìà –ê–ù–ê–õ–ò–ó –ò–°–¢–û–†–ò–ò")
    print("-" * 60)
    
    ema_wins = sum(1 for n, e in zip(eval_normal_history, eval_ema_history) if e < n)
    total_evals = len(eval_normal_history)
    
    print()
    print(f"   EMA –ª—É—á—à–µ –≤ {ema_wins}/{total_evals} —Ç–æ—á–∫–∞—Ö ({100*ema_wins/total_evals:.0f}%)")
    print()
    
    # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 –∏–∑–º–µ—Ä–µ–Ω–∏–π
    print("   –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 –∏–∑–º–µ—Ä–µ–Ω–∏–π:")
    for i, (n, e) in enumerate(zip(eval_normal_history[-5:], eval_ema_history[-5:])):
        marker = "‚úÖ" if e < n else "‚ö†Ô∏è"
        print(f"      {marker} Normal: {n:.4f}, EMA: {e:.4f}")
    
    # ========================================================================
    # –í—ã–≤–æ–¥—ã
    # ========================================================================
    print()
    print("=" * 60)
    print("üìù –í–´–í–û–î–´")
    print("=" * 60)
    print("""
   –ö–õ–Æ–ß–ï–í–û–ô –ò–ù–°–ê–ô–¢:
   
   EMA —Å–≥–ª–∞–∂–∏–≤–∞–µ—Ç –∫–æ–ª–µ–±–∞–Ω–∏—è –≤–µ—Å–æ–≤ –≤—ã–∑–≤–∞–Ω–Ω—ã–µ:
   - –®—É–º–æ–º –≤ –¥–∞–Ω–Ω—ã—Ö
   - –í—ã—Å–æ–∫–∏–º learning rate
   - –°—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å—é SGD
   
   –ö–û–ì–î–ê EMA –ü–û–ú–û–ì–ê–ï–¢:
   ‚úÖ –®—É–º–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã)
   ‚úÖ –í—ã—Å–æ–∫–∏–π learning rate
   ‚úÖ –î–ª–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (–Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏)
   ‚úÖ –ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ (–±–æ–ª—å—à–µ variance)
   
   –ö–û–ì–î–ê EMA –ù–ï –ü–û–ú–û–ì–ê–ï–¢:
   ‚ùå –ß–∏—Å—Ç—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
   ‚ùå –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏
   ‚ùå –ö–æ—Ä–æ—Ç–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ
""")
    print("=" * 60)
    
    return {
        "final_normal": final_normal,
        "final_ema": final_ema,
        "improvement_percent": improvement,
        "ema_win_rate": ema_wins / total_evals,
        "training_time": training_time,
    }


if __name__ == "__main__":
    results = run_benchmark()
