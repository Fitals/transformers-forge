"""
üî¨ EMA Benchmark ‚Äî Static vs Dynamic Decay
==========================================

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç:
1. –ë–µ–∑ EMA
2. EMA —Å–æ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–º decay
3. EMA —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º decay (–ù–û–í–ò–ù–ö–ê v1.0.4!)

–ó–∞–ø—É—Å–∫:
    python benchmarks/ema_benchmark.py

–ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: 30-60 —Å–µ–∫—É–Ω–¥ –Ω–∞ CPU
"""

import time
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))


def run_benchmark():
    """–ó–∞–ø—É—Å–∫ benchmark —Å—Ä–∞–≤–Ω–µ–Ω–∏—è Static vs Dynamic EMA."""
    
    print("=" * 70)
    print("üî¨ EMA BENCHMARK v3 ‚Äî Static vs Dynamic Decay")
    print("=" * 70)
    print()
    
    # –ò–º–ø–æ—Ä—Ç—ã
    print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫...")
    
    try:
        import torch
        import torch.nn as nn
        from torch.utils.data import DataLoader, TensorDataset
    except ImportError:
        print("‚ùå PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch")
        return
    
    try:
        from transformers.ema import (
            create_ema_state, 
            update_ema_state, 
            apply_ema_state,
            compute_dynamic_decay
        )
    except ImportError:
        print("‚ùå Transformers Forge –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install -e .")
        return
    
    print("‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    print()
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    HIDDEN_SIZE = 256
    NUM_LAYERS = 4
    BATCH_SIZE = 32
    NUM_SAMPLES = 2000
    NUM_STEPS = 300
    LEARNING_RATE = 2e-3  # –í—ã—Å–æ–∫–∏–π LR –¥–ª—è —à—É–º–∞
    
    # EMA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    STATIC_DECAY = 0.99
    MIN_DECAY = 0.9
    MAX_DECAY = 0.999
    
    print("‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"   Hidden size: {HIDDEN_SIZE}")
    print(f"   Layers: {NUM_LAYERS}")
    print(f"   Training steps: {NUM_STEPS}")
    print(f"   Learning rate: {LEARNING_RATE} (–≤—ã—Å–æ–∫–∏–π –¥–ª—è —à—É–º–∞)")
    print()
    print("üìä EMA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"   Static decay: {STATIC_DECAY}")
    print(f"   Dynamic decay: {MIN_DECAY} ‚Üí {MAX_DECAY}")
    print()
    
    # –ú–æ–¥–µ–ª—å
    class SimpleTransformer(nn.Module):
        def __init__(self, hidden_size, num_layers):
            super().__init__()
            self.embedding = nn.Linear(hidden_size, hidden_size)
            self.layers = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(hidden_size, hidden_size * 4),
                    nn.GELU(),
                    nn.Linear(hidden_size * 4, hidden_size),
                    nn.LayerNorm(hidden_size)
                )
                for _ in range(num_layers)
            ])
            self.head = nn.Linear(hidden_size, hidden_size)
        
        def forward(self, x):
            x = self.embedding(x)
            for layer in self.layers:
                x = x + layer(x)
            return self.head(x)
    
    # –î–∞–Ω–Ω—ã–µ
    print("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
    torch.manual_seed(42)
    X = torch.randn(NUM_SAMPLES, HIDDEN_SIZE)
    Y = torch.sin(X) * 0.5 + torch.randn_like(X) * 0.2
    
    dataset = TensorDataset(X, Y)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    
    X_eval = torch.randn(500, HIDDEN_SIZE)
    Y_eval = torch.sin(X_eval) * 0.5
    
    def evaluate(model, X_eval, Y_eval):
        model.eval()
        with torch.no_grad():
            pred = model(X_eval)
            loss = nn.MSELoss()(pred, Y_eval)
        model.train()
        return loss.item()
    
    def train_with_ema(use_dynamic: bool, tag: str):
        """–û–±—É—á–µ–Ω–∏–µ —Å EMA (—Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–º –∏–ª–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º)."""
        model = SimpleTransformer(HIDDEN_SIZE, NUM_LAYERS)
        optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
        criterion = nn.MSELoss()
        
        ema_state = create_ema_state(model)
        
        eval_normal_history = []
        eval_ema_history = []
        decay_history = []
        
        step = 0
        for epoch in range(20):
            for batch_x, batch_y in dataloader:
                optimizer.zero_grad()
                pred = model(batch_x)
                loss = criterion(pred, batch_y)
                loss.backward()
                optimizer.step()
                
                # –í—ã—á–∏—Å–ª—è–µ–º decay
                if use_dynamic:
                    decay = compute_dynamic_decay(
                        current_step=step,
                        total_steps=NUM_STEPS,
                        min_decay=MIN_DECAY,
                        max_decay=MAX_DECAY,
                        schedule="linear"
                    )
                else:
                    decay = STATIC_DECAY
                
                # –û–±–Ω–æ–≤–ª—è–µ–º EMA
                update_ema_state(model, ema_state, decay=decay)
                
                if step % 50 == 0:
                    eval_normal = evaluate(model, X_eval, Y_eval)
                    eval_normal_history.append(eval_normal)
                    
                    backup = apply_ema_state(model, ema_state)
                    eval_ema = evaluate(model, X_eval, Y_eval)
                    eval_ema_history.append(eval_ema)
                    apply_ema_state(model, backup)
                    
                    decay_history.append(decay)
                    
                    diff = ((eval_normal - eval_ema) / eval_normal) * 100 if eval_normal > 0 else 0
                    marker = "‚úÖ" if eval_ema < eval_normal else "‚ö†Ô∏è"
                    
                    print(f"   [{tag}] Step {step:3d} | decay={decay:.3f} | Normal: {eval_normal:.4f} | EMA: {eval_ema:.4f} | {marker} {diff:+.1f}%")
                
                step += 1
                if step >= NUM_STEPS:
                    break
            if step >= NUM_STEPS:
                break
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        final_normal = evaluate(model, X_eval, Y_eval)
        backup = apply_ema_state(model, ema_state)
        final_ema = evaluate(model, X_eval, Y_eval)
        
        ema_wins = sum(1 for n, e in zip(eval_normal_history, eval_ema_history) if e < n)
        
        return {
            "final_normal": final_normal,
            "final_ema": final_ema,
            "improvement": ((final_normal - final_ema) / final_normal) * 100,
            "ema_win_rate": ema_wins / len(eval_normal_history) if eval_normal_history else 0,
            "final_decay": decay_history[-1] if decay_history else STATIC_DECAY,
        }
    
    # ========================================================================
    # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
    # ========================================================================
    
    print()
    print("-" * 70)
    print("üî¥ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1: EMA —Å–æ –°–¢–ê–¢–ò–ß–ï–°–ö–ò–ú decay")
    print("-" * 70)
    static_results = train_with_ema(use_dynamic=False, tag="STATIC")
    
    print()
    print("-" * 70)
    print("üü¢ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 2: EMA —Å –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ú decay")
    print("-" * 70)
    dynamic_results = train_with_ema(use_dynamic=True, tag="DYNAMIC")
    
    # ========================================================================
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    # ========================================================================
    print()
    print("=" * 70)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ BENCHMARK")
    print("=" * 70)
    print()
    
    print(f"   {'–ú–µ—Ç—Ä–∏–∫–∞':<30} {'Static EMA':<15} {'Dynamic EMA':<15}")
    print(f"   {'-'*30} {'-'*15} {'-'*15}")
    
    print(f"   {'Final Eval (Normal)':<30} {static_results['final_normal']:<15.4f} {dynamic_results['final_normal']:<15.4f}")
    print(f"   {'Final Eval (EMA)':<30} {static_results['final_ema']:<15.4f} {dynamic_results['final_ema']:<15.4f}")
    print(f"   {'EMA Improvement':<30} {static_results['improvement']:>+14.1f}% {dynamic_results['improvement']:>+14.1f}%")
    print(f"   {'EMA Win Rate':<30} {static_results['ema_win_rate']*100:>14.0f}% {dynamic_results['ema_win_rate']*100:>14.0f}%")
    print(f"   {'Final Decay':<30} {static_results['final_decay']:<15.3f} {dynamic_results['final_decay']:<15.3f}")
    
    print()
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    if dynamic_results['improvement'] > static_results['improvement']:
        diff = dynamic_results['improvement'] - static_results['improvement']
        print(f"   ‚úÖ Dynamic EMA –ª—É—á—à–µ –Ω–∞ {diff:.1f}%!")
    elif static_results['improvement'] > dynamic_results['improvement']:
        diff = static_results['improvement'] - dynamic_results['improvement']
        print(f"   ‚ö†Ô∏è Static EMA –ª—É—á—à–µ –Ω–∞ {diff:.1f}%")
    else:
        print(f"   ‚ûñ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã")
    
    if dynamic_results['ema_win_rate'] > static_results['ema_win_rate']:
        print(f"   ‚úÖ Dynamic EMA –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç —á–∞—â–µ!")
    
    print()
    print("=" * 70)
    print("üìù –í–´–í–û–î–´")
    print("=" * 70)
    print("""
   DYNAMIC DECAY —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ—Ç—Å—Ç–∞–≤–∞–Ω–∏—è EMA:
   
   üìâ Static decay (0.99):
      - –ù–∞—á–∞–ª–æ: EMA —Å–∏–ª—å–Ω–æ –æ—Ç—Å—Ç–∞—ë—Ç (–ø–æ–º–Ω–∏—Ç –ø–ª–æ—Ö–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞)
      - –ö–æ–Ω–µ—Ü: EMA –º–æ–∂–µ—Ç –Ω–µ –¥–æ–≥–Ω–∞—Ç—å –º–æ–¥–µ–ª—å
   
   üìà Dynamic decay (0.9 ‚Üí 0.999):
      - –ù–∞—á–∞–ª–æ: decay=0.9 (–±—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ —Ç–µ–∫—É—â–∏–º –≤–µ—Å–∞–º)
      - –ö–æ–Ω–µ—Ü: decay=0.999 (—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ)
   
   –ö–û–ì–î–ê –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨:
   ‚úÖ Dynamic decay ‚Äî –¥–ª—è –ª—é–±–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
   ‚úÖ –û—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫
   ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –¥–ª–∏–Ω–µ –æ–±—É—á–µ–Ω–∏—è
""")
    print("=" * 70)
    
    return {
        "static": static_results,
        "dynamic": dynamic_results,
    }


if __name__ == "__main__":
    results = run_benchmark()
