# Copyright 2024 Transformers Forge Contributors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Learning Rate Finder ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ learning rate.

–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ä–∞–±–æ—Ç–µ Leslie Smith (2015):
"Cyclical Learning Rates for Training Neural Networks"

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    >>> from transformers.lr_finder import LRFinder
    >>> finder = LRFinder(model, train_dataloader)
    >>> optimal_lr = finder.find()
    >>> print(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π LR: {optimal_lr}")

    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫
    >>> finder.plot("lr_finder.png")
"""

import copy
import math
import warnings
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

from .utils import logging


logger = logging.get_logger(__name__)


# =============================================================================
# Data Classes
# =============================================================================


@dataclass
class LRFinderResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ learning rate.
    
    Attributes:
        optimal_lr: –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π learning rate.
        min_lr: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–∏—Ä—É–µ–º—ã–π LR.
        max_lr: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–∏—Ä—É–µ–º—ã–π LR.
        num_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Ç–µ—Å—Ç–∞.
        lrs: –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö LR.
        losses: –°–ø–∏—Å–æ–∫ loss –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ.
        smoothed_losses: –°–≥–ª–∞–∂–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è loss.
        best_lr_idx: –ò–Ω–¥–µ–∫—Å —Ç–æ—á–∫–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º loss.
        suggestion_method: –ú–µ—Ç–æ–¥ –≤—ã–±–æ—Ä–∞ ("steepest_gradient" –∏–ª–∏ "minimum").
    """
    optimal_lr: float
    min_lr: float
    max_lr: float
    num_steps: int
    lrs: List[float] = field(default_factory=list)
    losses: List[float] = field(default_factory=list)
    smoothed_losses: List[float] = field(default_factory=list)
    best_lr_idx: int = 0
    suggestion_method: str = "steepest_gradient"
    
    def __repr__(self) -> str:
        return (
            f"LRFinderResult(\n"
            f"  optimal_lr={self.optimal_lr:.2e},\n"
            f"  range=[{self.min_lr:.2e}, {self.max_lr:.2e}],\n"
            f"  num_steps={self.num_steps},\n"
            f"  method='{self.suggestion_method}'\n"
            f")"
        )


# =============================================================================
# LR Finder
# =============================================================================


class LRFinder:
    """
    Learning Rate Finder ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ learning rate.
    
    –ê–ª–≥–æ—Ä–∏—Ç–º (Leslie Smith, 2015):
    1. –ù–∞—á–∏–Ω–∞–µ—Ç —Å –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–æ–≥–æ LR
    2. –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç LR –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ
    3. –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç loss –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ
    4. –ù–∞—Ö–æ–¥–∏—Ç LR –≥–¥–µ loss –º–∏–Ω–∏–º–∞–ª–µ–Ω –∏–ª–∏ —Ä–∞—Å—Ç—ë—Ç –±—ã—Å—Ç—Ä–µ–µ –≤—Å–µ–≥–æ
    5. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç LR –Ω–µ–º–Ω–æ–≥–æ –Ω–∏–∂–µ —ç—Ç–æ–π —Ç–æ—á–∫–∏
    
    Args:
        model: PyTorch –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
        train_dataloader: DataLoader —Å –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
        optimizer: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é AdamW).
        criterion: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è loss.
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ("auto", "cuda", "cpu").
        
    Example:
        >>> from transformers import AutoModelForCausalLM
        >>> from transformers.lr_finder import LRFinder
        >>> 
        >>> model = AutoModelForCausalLM.from_pretrained("gpt2")
        >>> finder = LRFinder(model, train_dataloader)
        >>> result = finder.find()
        >>> print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π LR: {result.optimal_lr}")
        >>> finder.plot("lr_curve.png")
    """
    
    def __init__(
        self,
        model: "torch.nn.Module",
        train_dataloader: "torch.utils.data.DataLoader",
        optimizer: Optional["torch.optim.Optimizer"] = None,
        criterion: Optional[callable] = None,
        device: str = "auto",
    ):
        try:
            import torch
        except ImportError:
            raise ImportError(
                "LRFinder —Ç—Ä–µ–±—É–µ—Ç PyTorch. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ –∫–æ–º–∞–Ω–¥–æ–π: pip install torch"
            )
        
        self.torch = torch
        self.model = model
        self.train_dataloader = train_dataloader
        self.criterion = criterion
        self.device = self._get_device(device)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self._original_state = copy.deepcopy(model.state_dict())
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.result: Optional[LRFinderResult] = None
        self._lrs: List[float] = []
        self._losses: List[float] = []
        
        # –°–æ–∑–¥–∞—ë–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
        if optimizer is None:
            self._optimizer_class = torch.optim.AdamW
            self._optimizer_kwargs = {"weight_decay": 0.01}
        else:
            self._optimizer = optimizer
            self._optimizer_class = None
        
        logger.info(
            f"LRFinder initialized: "
            f"model={type(model).__name__}, "
            f"device={self.device}"
        )
    
    def _get_device(self, device: str) -> "torch.device":
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
        if device == "auto":
            if self.torch.cuda.is_available():
                return self.torch.device("cuda")
            elif hasattr(self.torch.backends, "mps") and self.torch.backends.mps.is_available():
                return self.torch.device("mps")
            else:
                return self.torch.device("cpu")
        return self.torch.device(device)
    
    def _create_optimizer(self, lr: float) -> "torch.optim.Optimizer":
        """–°–æ–∑–¥–∞—ë—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –∑–∞–¥–∞–Ω–Ω—ã–º LR."""
        if self._optimizer_class is not None:
            return self._optimizer_class(
                self.model.parameters(),
                lr=lr,
                **self._optimizer_kwargs
            )
        else:
            # –û–±–Ω–æ–≤–ª—è–µ–º LR —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
            for param_group in self._optimizer.param_groups:
                param_group["lr"] = lr
            return self._optimizer
    
    def _compute_loss(
        self,
        batch: Dict[str, Any],
    ) -> "torch.Tensor":
        """–í—ã—á–∏—Å–ª—è–µ—Ç loss –¥–ª—è –±–∞—Ç—á–∞."""
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –±–∞—Ç—á –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        batch = {
            k: v.to(self.device) if hasattr(v, "to") else v
            for k, v in batch.items()
        }
        
        # Forward pass
        outputs = self.model(**batch)
        
        # –ü–æ–ª—É—á–∞–µ–º loss
        if self.criterion is not None:
            loss = self.criterion(outputs, batch)
        elif hasattr(outputs, "loss") and outputs.loss is not None:
            loss = outputs.loss
        else:
            raise ValueError(
                "–ú–æ–¥–µ–ª—å –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç loss. "
                "–ü–µ—Ä–µ–¥–∞–π—Ç–µ criterion –≤ LRFinder –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª—å —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º loss."
            )
        
        return loss
    
    def _smooth_losses(
        self,
        losses: List[float],
        beta: float = 0.98,
    ) -> List[float]:
        """–°–≥–ª–∞–∂–∏–≤–∞–µ—Ç loss —Å –ø–æ–º–æ—â—å—é exponential moving average."""
        smoothed = []
        avg_loss = 0.0
        
        for i, loss in enumerate(losses):
            avg_loss = beta * avg_loss + (1 - beta) * loss
            # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è (bias correction)
            smoothed.append(avg_loss / (1 - beta ** (i + 1)))
        
        return smoothed
    
    def _find_steep_gradient(
        self,
        lrs: List[float],
        smoothed_losses: List[float],
    ) -> int:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–∫—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–º loss."""
        if len(lrs) < 3:
            return 0
        
        gradients = []
        for i in range(1, len(smoothed_losses)):
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç –≤ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ LR
            grad = (smoothed_losses[i] - smoothed_losses[i - 1]) / (
                math.log10(lrs[i]) - math.log10(lrs[i - 1])
            )
            gradients.append(grad)
        
        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ—á–∫—É —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º (—Å–∞–º—ã–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º) –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–º
        min_grad_idx = gradients.index(min(gradients))
        
        return min_grad_idx
    
    def find(
        self,
        min_lr: float = 1e-8,
        max_lr: float = 1e-1,
        num_steps: int = 100,
        smooth_factor: float = 0.98,
        divergence_threshold: float = 4.0,
        suggestion_method: str = "steepest_gradient",
    ) -> LRFinderResult:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ learning rate.
        
        Args:
            min_lr: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π LR –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
            max_lr: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π LR –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
            num_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Ç–µ—Å—Ç–∞.
            smooth_factor: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è loss (0-1).
            divergence_threshold: –ü–æ—Ä–æ–≥ –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–∏ –≤–∑—Ä—ã–≤–µ loss.
            suggestion_method: –ú–µ—Ç–æ–¥ –≤—ã–±–æ—Ä–∞ LR:
                - "steepest_gradient": LR –≥–¥–µ loss –ø–∞–¥–∞–µ—Ç –±—ã—Å—Ç—Ä–µ–µ –≤—Å–µ–≥–æ
                - "minimum": LR —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º loss
                
        Returns:
            LRFinderResult —Å —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–º LR –∏ –∏—Å—Ç–æ—Ä–∏–µ–π.
            
        Example:
            >>> result = finder.find(min_lr=1e-7, max_lr=1e-2, num_steps=50)
            >>> print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π LR: {result.optimal_lr:.2e}")
        """
        logger.info(
            f"Starting LR search: "
            f"range=[{min_lr:.2e}, {max_lr:.2e}], "
            f"steps={num_steps}"
        )
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
        self.model.load_state_dict(copy.deepcopy(self._original_state))
        self.model.to(self.device)
        self.model.train()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å –¥–ª—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞ LR
        lr_mult = (max_lr / min_lr) ** (1 / num_steps)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        current_lr = min_lr
        optimizer = self._create_optimizer(current_lr)
        
        self._lrs = []
        self._losses = []
        best_loss = float("inf")
        
        # –°–æ–∑–¥–∞—ë–º –∏—Ç–µ—Ä–∞—Ç–æ—Ä –ø–æ –¥–∞–Ω–Ω—ã–º
        data_iter = iter(self.train_dataloader)
        
        print(f"\nüîç LR Finder: –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ learning rate...")
        print(f"   –î–∏–∞–ø–∞–∑–æ–Ω: [{min_lr:.2e}, {max_lr:.2e}]")
        print(f"   –®–∞–≥–∏: {num_steps}")
        print()
        
        for step in range(num_steps):
            # –ü–æ–ª—É—á–∞–µ–º –±–∞—Ç—á (—Å –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–º –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
            try:
                batch = next(data_iter)
            except StopIteration:
                data_iter = iter(self.train_dataloader)
                batch = next(data_iter)
            
            # Forward + backward
            optimizer.zero_grad()
            
            try:
                loss = self._compute_loss(batch)
                loss.backward()
                optimizer.step()
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    warnings.warn(
                        f"OOM –Ω–∞ —à–∞–≥–µ {step} —Å LR={current_lr:.2e}. "
                        "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å batch size."
                    )
                    break
                raise
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            loss_value = loss.item()
            self._lrs.append(current_lr)
            self._losses.append(loss_value)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–∏–π loss
            if loss_value < best_loss:
                best_loss = loss_value
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –≤–∑—Ä—ã–≤ loss
            if loss_value > best_loss * divergence_threshold:
                logger.info(
                    f"Loss diverged at step {step} "
                    f"(loss={loss_value:.4f} > {divergence_threshold}x best)"
                )
                print(f"   ‚ö†Ô∏è Loss –≤–∑–æ—Ä–≤–∞–ª—Å—è –Ω–∞ —à–∞–≥–µ {step}, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–∏—Å–∫")
                break
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å
            if (step + 1) % 10 == 0 or step == 0:
                print(f"   –®–∞–≥ {step + 1}/{num_steps}: LR={current_lr:.2e}, Loss={loss_value:.4f}")
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º LR
            current_lr *= lr_mult
            for param_group in optimizer.param_groups:
                param_group["lr"] = current_lr
        
        # –°–≥–ª–∞–∂–∏–≤–∞–µ–º losses
        smoothed = self._smooth_losses(self._losses, smooth_factor)
        
        # –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π LR
        if suggestion_method == "steepest_gradient":
            best_idx = self._find_steep_gradient(self._lrs, smoothed)
            # –ë–µ—Ä—ë–º LR –Ω–µ–º–Ω–æ–≥–æ –ª–µ–≤–µ–µ —Ç–æ—á–∫–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞
            optimal_idx = max(0, best_idx - 1)
        else:  # minimum
            optimal_idx = smoothed.index(min(smoothed))
        
        optimal_lr = self._lrs[optimal_idx]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–±–µ—Ä—ë–º LR –Ω–∞ –ø–æ—Ä—è–¥–æ–∫ –º–µ–Ω—å—à–µ)
        suggested_lr = optimal_lr / 10
        
        # –°–æ–∑–¥–∞—ë–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.result = LRFinderResult(
            optimal_lr=suggested_lr,
            min_lr=min_lr,
            max_lr=max_lr,
            num_steps=len(self._lrs),
            lrs=self._lrs.copy(),
            losses=self._losses.copy(),
            smoothed_losses=smoothed,
            best_lr_idx=optimal_idx,
            suggestion_method=suggestion_method,
        )
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
        self.model.load_state_dict(copy.deepcopy(self._original_state))
        
        print()
        print(f"   ‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π LR: {suggested_lr:.2e}")
        print(f"      (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ {len(self._lrs)} —à–∞–≥–æ–≤)")
        print()
        
        logger.info(f"LR search complete: optimal_lr={suggested_lr:.2e}")
        
        return self.result
    
    def plot(
        self,
        output_path: Optional[str] = None,
        log_scale: bool = True,
        show_suggestion: bool = True,
        skip_start: int = 5,
        skip_end: int = 5,
    ) -> Optional[str]:
        """
        –°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ loss –æ—Ç learning rate.
        
        Args:
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ (None = –ø–æ–∫–∞–∑–∞—Ç—å).
            log_scale: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫—É—é —à–∫–∞–ª—É –¥–ª—è LR.
            show_suggestion: –ü–æ–∫–∞–∑–∞—Ç—å –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—É—é –ª–∏–Ω–∏—é –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–≥–æ LR.
            skip_start: –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø–µ—Ä–≤—ã–µ N —Ç–æ—á–µ–∫ (–æ–±—ã—á–Ω–æ —à—É–º–Ω—ã–µ).
            skip_end: –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Ç–æ—á–µ–∫ (–æ–±—ã—á–Ω–æ –≤–∑—Ä—ã–≤–Ω—ã–µ).
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–º—É –≥—Ä–∞—Ñ–∏–∫—É –∏–ª–∏ None.
            
        Example:
            >>> finder.plot("lr_curve.png")
        """
        if self.result is None:
            raise ValueError(
                "–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ find() –¥–ª—è –ø–æ–∏—Å–∫–∞ LR."
            )
        
        try:
            import matplotlib
            matplotlib.use("Agg")  # –î–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ GUI
            import matplotlib.pyplot as plt
        except ImportError:
            warnings.warn(
                "matplotlib –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ –∫–æ–º–∞–Ω–¥–æ–π: pip install matplotlib"
            )
            return None
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        lrs = self.result.lrs[skip_start:-skip_end] if skip_end > 0 else self.result.lrs[skip_start:]
        losses = self.result.smoothed_losses[skip_start:-skip_end] if skip_end > 0 else self.result.smoothed_losses[skip_start:]
        
        # –°–æ–∑–¥–∞—ë–º –≥—Ä–∞—Ñ–∏–∫
        fig, ax = plt.subplots(figsize=(10, 6))
        
        ax.plot(lrs, losses, linewidth=2, color="#2E86AB", label="Loss (smoothed)")
        
        if show_suggestion and self.result.optimal_lr:
            ax.axvline(
                x=self.result.optimal_lr,
                color="#E94F37",
                linestyle="--",
                linewidth=2,
                label=f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π LR: {self.result.optimal_lr:.2e}"
            )
        
        if log_scale:
            ax.set_xscale("log")
        
        ax.set_xlabel("Learning Rate", fontsize=12)
        ax.set_ylabel("Loss", fontsize=12)
        ax.set_title("LR Finder ‚Äî Transformers Forge", fontsize=14, fontweight="bold")
        ax.legend(loc="upper left")
        ax.grid(True, alpha=0.3)
        
        # –°—Ç–∏–ª–∏–∑–∞—Ü–∏—è
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)
        
        plt.tight_layout()
        
        if output_path:
            path = Path(output_path)
            path.parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(path, dpi=150, bbox_inches="tight")
            plt.close()
            logger.info(f"LR plot saved to {path}")
            return str(path)
        else:
            plt.show()
            return None
    
    def reset(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏."""
        self.model.load_state_dict(copy.deepcopy(self._original_state))
        self.result = None
        self._lrs = []
        self._losses = []
        logger.info("LRFinder reset")
    
    def get_suggestion(self) -> float:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π LR –±–µ–∑ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –ø–æ–∏—Å–∫–∞."""
        if self.result is None:
            raise ValueError(
                "–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ find() –¥–ª—è –ø–æ–∏—Å–∫–∞ LR."
            )
        return self.result.optimal_lr


# =============================================================================
# Convenience functions
# =============================================================================


def find_optimal_lr(
    model: "torch.nn.Module",
    train_dataloader: "torch.utils.data.DataLoader",
    min_lr: float = 1e-8,
    max_lr: float = 1e-1,
    num_steps: int = 100,
    device: str = "auto",
    plot_path: Optional[str] = None,
) -> float:
    """
    –ë—ã—Å—Ç—Ä—ã–π —Å–ø–æ—Å–æ–± –Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π learning rate.
    
    Args:
        model: PyTorch –º–æ–¥–µ–ª—å.
        train_dataloader: DataLoader —Å –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
        min_lr: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π LR –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
        max_lr: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π LR –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
        num_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Ç–µ—Å—Ç–∞.
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ("auto", "cuda", "cpu").
        plot_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ).
        
    Returns:
        –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π learning rate.
        
    Example:
        >>> optimal_lr = find_optimal_lr(model, train_dataloader)
        >>> print(f"–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ LR: {optimal_lr:.2e}")
    """
    finder = LRFinder(model, train_dataloader, device=device)
    result = finder.find(min_lr=min_lr, max_lr=max_lr, num_steps=num_steps)
    
    if plot_path:
        finder.plot(plot_path)
    
    return result.optimal_lr
