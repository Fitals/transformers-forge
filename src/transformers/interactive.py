# Copyright 2024 Community Enhanced Contributors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Interactive Model Manager for Transformers Forge.

Provides an interactive console interface for:
- Scanning and selecting local models
- Scanning and validating datasets
- Running inference (chat)
- Setting up fine-tuning

Usage:
    from transformers.interactive import InteractiveModelManager
    
    manager = InteractiveModelManager(
        models_dir="./models",
        datasets_dir="./datasets"
    )
    manager.run()

Or from command line:
    python -m transformers.interactive --models ./models --datasets ./datasets
"""

import json
import os
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from .utils import logging

logger = logging.get_logger(__name__)


# =============================================================================
# Data Classes
# =============================================================================


@dataclass
class ModelInfo:
    """Information about a discovered model."""
    name: str
    path: str
    size_gb: float
    model_type: str = "Unknown"
    num_parameters: Optional[str] = None
    has_tokenizer: bool = False
    has_safetensors: bool = False
    has_pytorch: bool = False


@dataclass
class DatasetInfo:
    """Information about a discovered dataset."""
    name: str
    path: str
    size_mb: float
    num_lines: int = 0
    format: str = "unknown"  # jsonl, json, csv


@dataclass
class ValidationResult:
    """Result of dataset validation."""
    valid: bool
    total_lines: int = 0
    valid_lines: int = 0
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    sample_line: Optional[str] = None


# =============================================================================
# Interactive Model Manager
# =============================================================================


class InteractiveModelManager:
    """
    Interactive console interface for model management.
    
    Provides a user-friendly way to:
    - Browse and select local models
    - Validate and select datasets
    - Run inference or set up fine-tuning
    
    Args:
        models_dir: Directory containing HuggingFace models
        datasets_dir: Directory containing training datasets
        
    Example:
        >>> from transformers.interactive import InteractiveModelManager
        >>> manager = InteractiveModelManager(
        ...     models_dir="./models",
        ...     datasets_dir="./datasets"
        ... )
        >>> manager.run()
    """
    
    def __init__(
        self,
        models_dir: str = "./models",
        datasets_dir: str = "./datasets"
    ):
        self.models_dir = Path(models_dir)
        self.datasets_dir = Path(datasets_dir)
        
        self.selected_model: Optional[ModelInfo] = None
        self.selected_dataset: Optional[DatasetInfo] = None
    
    # =========================================================================
    # Model Scanning
    # =========================================================================
    
    def scan_models(self) -> List[ModelInfo]:
        """
        Scan models directory for HuggingFace models.
        
        Looks for directories containing config.json (HF format).
        Excludes GGUF files as they don't support fine-tuning.
        
        Returns:
            List of ModelInfo objects for discovered models.
        """
        models = []
        
        if not self.models_dir.exists():
            return models
        
        for item in self.models_dir.iterdir():
            if not item.is_dir():
                continue
            
            config_path = item / "config.json"
            if not config_path.exists():
                continue
            
            # Skip if contains GGUF files
            gguf_files = list(item.glob("*.gguf"))
            if gguf_files:
                continue
            
            # Get model info
            model_info = self._get_model_info(item)
            if model_info:
                models.append(model_info)
        
        return sorted(models, key=lambda m: m.name)
    
    def _get_model_info(self, model_path: Path) -> Optional[ModelInfo]:
        """Extract model information from directory."""
        try:
            # Read config
            config_path = model_path / "config.json"
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)
            
            # Calculate size
            total_size = sum(
                f.stat().st_size for f in model_path.rglob("*") if f.is_file()
            )
            size_gb = total_size / (1024 ** 3)
            
            # Get model type
            model_type = config.get("model_type", "Unknown")
            architectures = config.get("architectures", [])
            if architectures:
                model_type = architectures[0]
            
            # Check for files
            has_tokenizer = (model_path / "tokenizer.json").exists() or \
                           (model_path / "tokenizer_config.json").exists()
            has_safetensors = bool(list(model_path.glob("*.safetensors")))
            has_pytorch = bool(list(model_path.glob("*.bin")))
            
            # Estimate parameters
            num_params = None
            if "num_parameters" in config:
                num_params = self._format_params(config["num_parameters"])
            elif "hidden_size" in config and "num_hidden_layers" in config:
                # Rough estimate
                h = config["hidden_size"]
                l = config["num_hidden_layers"]
                v = config.get("vocab_size", 32000)
                estimated = l * 12 * h * h + v * h
                num_params = self._format_params(estimated)
            
            return ModelInfo(
                name=model_path.name,
                path=str(model_path),
                size_gb=size_gb,
                model_type=model_type,
                num_parameters=num_params,
                has_tokenizer=has_tokenizer,
                has_safetensors=has_safetensors,
                has_pytorch=has_pytorch
            )
        except Exception as e:
            logger.warning(f"Failed to read model {model_path}: {e}")
            return None
    
    def _format_params(self, count: int) -> str:
        """Format parameter count (1.5B, 7M, etc.)."""
        if count >= 1e9:
            return f"{count / 1e9:.1f}B"
        elif count >= 1e6:
            return f"{count / 1e6:.1f}M"
        else:
            return f"{count / 1e3:.1f}K"
    
    # =========================================================================
    # Dataset Scanning & Validation
    # =========================================================================
    
    def scan_datasets(self) -> List[DatasetInfo]:
        """
        Scan datasets directory for training files.
        
        Looks for .jsonl, .json files.
        
        Returns:
            List of DatasetInfo objects.
        """
        datasets = []
        
        if not self.datasets_dir.exists():
            return datasets
        
        for ext in ["*.jsonl", "*.json"]:
            for file_path in self.datasets_dir.glob(ext):
                if file_path.is_file():
                    info = self._get_dataset_info(file_path)
                    if info:
                        datasets.append(info)
        
        return sorted(datasets, key=lambda d: d.name)
    
    def _get_dataset_info(self, file_path: Path) -> Optional[DatasetInfo]:
        """Extract dataset information."""
        try:
            size_mb = file_path.stat().st_size / (1024 ** 2)
            
            # Count lines
            num_lines = 0
            with open(file_path, "r", encoding="utf-8") as f:
                for _ in f:
                    num_lines += 1
            
            # Determine format
            fmt = "jsonl" if file_path.suffix == ".jsonl" else "json"
            
            return DatasetInfo(
                name=file_path.name,
                path=str(file_path),
                size_mb=size_mb,
                num_lines=num_lines,
                format=fmt
            )
        except Exception as e:
            logger.warning(f"Failed to read dataset {file_path}: {e}")
            return None
    
    def validate_dataset(self, dataset_path: str, max_check: int = 100) -> ValidationResult:
        """
        Validate dataset format and content.
        
        Checks:
        - Valid JSON/JSONL format
        - Presence of 'messages' field
        - Each message has 'role' and 'content'
        - Valid roles (system, user, assistant)
        
        Args:
            dataset_path: Path to dataset file
            max_check: Maximum lines to check (for large files)
            
        Returns:
            ValidationResult with errors and warnings.
        """
        errors = []
        warnings = []
        valid_lines = 0
        total_lines = 0
        sample_line = None
        
        valid_roles = {"system", "user", "assistant"}
        
        try:
            with open(dataset_path, "r", encoding="utf-8") as f:
                for i, line in enumerate(f):
                    total_lines += 1
                    line = line.strip()
                    
                    if not line:
                        continue
                    
                    # Only check first max_check lines
                    if i >= max_check:
                        continue
                    
                    try:
                        data = json.loads(line)
                    except json.JSONDecodeError as e:
                        errors.append(f"Line {i+1}: Invalid JSON - {e}")
                        continue
                    
                    # Check for messages field
                    if "messages" not in data:
                        errors.append(f"Line {i+1}: Missing 'messages' field")
                        continue
                    
                    messages = data["messages"]
                    if not isinstance(messages, list):
                        errors.append(f"Line {i+1}: 'messages' must be a list")
                        continue
                    
                    if len(messages) == 0:
                        warnings.append(f"Line {i+1}: Empty messages list")
                        continue
                    
                    # Check each message
                    line_valid = True
                    for j, msg in enumerate(messages):
                        if not isinstance(msg, dict):
                            errors.append(f"Line {i+1}, msg {j}: Not a dict")
                            line_valid = False
                            continue
                        
                        if "role" not in msg:
                            errors.append(f"Line {i+1}, msg {j}: Missing 'role'")
                            line_valid = False
                        elif msg["role"] not in valid_roles:
                            warnings.append(f"Line {i+1}, msg {j}: Unknown role '{msg['role']}'")
                        
                        if "content" not in msg:
                            errors.append(f"Line {i+1}, msg {j}: Missing 'content'")
                            line_valid = False
                    
                    if line_valid:
                        valid_lines += 1
                        if sample_line is None:
                            sample_line = line[:500]  # First 500 chars
            
        except Exception as e:
            errors.append(f"Failed to read file: {e}")
        
        return ValidationResult(
            valid=len(errors) == 0,
            total_lines=total_lines,
            valid_lines=valid_lines,
            errors=errors[:10],  # Limit errors shown
            warnings=warnings[:5],
            sample_line=sample_line
        )
    
    def show_dataset_example(self):
        """Show example of correct dataset format."""
        print()
        print("=" * 70)
        print("ğŸ“„ Ğ¤ĞĞ ĞœĞĞ¢ Ğ”ĞĞ¢ĞĞ¡Ğ•Ğ¢Ğ (ChatML JSONL)")
        print("=" * 70)
        print()
        print("Ğ¤Ğ°Ğ¹Ğ»: train.jsonl (Ğ¾Ğ´Ğ½Ğ° ÑÑ‚Ñ€Ğ¾ĞºĞ° = Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€)")
        print()
        print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print("â”‚ ĞĞ‘Ğ«Ğ§ĞĞ«Ğ™ Ğ¤ĞĞ ĞœĞĞ¢:                                                     â”‚")
        print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
        print('â”‚ {"messages": [{"role": "system", "content": "Ğ¢Ñ‹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº."},        â”‚')
        print('â”‚ {"role": "user", "content": "Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ"}, {"role": "assistant",        â”‚')
        print('â”‚ "content": "ĞÑ‚Ğ²ĞµÑ‚"}]}                                               â”‚')
        print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        print()
        print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print("â”‚ Ğ¡ REASONING (<think>):                                              â”‚")
        print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
        print('â”‚ {"messages": [..., {"role": "assistant", "content":                 â”‚')
        print('â”‚ "<think>Ğ Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ...</think>\\n\\nĞÑ‚Ğ²ĞµÑ‚"}]}                         â”‚')
        print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        print()
        print("âš ï¸  Ğ’Ğ°Ğ¶Ğ½Ğ¾:")
        print("   â€¢ ĞšĞ°Ğ¶Ğ´Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ° â€” Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ JSON Ğ¾Ğ±ÑŠĞµĞºÑ‚")
        print("   â€¢ ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ: messages, role, content")
        print("   â€¢ Ğ Ğ¾Ğ»Ğ¸: system, user, assistant")
        print("=" * 70)
        print()
    
    # =========================================================================
    # UI Methods
    # =========================================================================
    
    def _print_header(self, title: str):
        """Print a formatted header."""
        print()
        print("=" * 70)
        print(f"  {title}")
        print("=" * 70)
    
    def _print_disclaimer(self, title: str, lines: List[str]):
        """Print a disclaimer box."""
        print()
        print("=" * 70)
        print(f"âš ï¸  {title}")
        print("=" * 70)
        for line in lines:
            print(f"   {line}")
        print("=" * 70)
    
    def show_models_menu(self, models: List[ModelInfo]) -> Optional[ModelInfo]:
        """Display models selection menu."""
        self._print_header("ğŸ“‚ ĞĞ‘ĞĞĞ Ğ£Ğ–Ğ•ĞĞĞ«Ğ• ĞœĞĞ”Ğ•Ğ›Ğ˜")
        
        if not models:
            print()
            print("   âŒ ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹!")
            print()
            print(f"   ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ¿Ğ°Ğ¿ĞºÑƒ: {self.models_dir.absolute()}")
            print()
            print("   ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹:")
            print("   ./models/")
            print("   â””â”€â”€ Qwen2.5-3B/")
            print("       â”œâ”€â”€ config.json")
            print("       â”œâ”€â”€ model.safetensors")
            print("       â””â”€â”€ tokenizer.json")
            print()
            return None
        
        print(f"\n   ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: {len(models)}\n")
        
        for i, model in enumerate(models, 1):
            status = "âœ…" if model.has_safetensors or model.has_pytorch else "âš ï¸"
            print(f"   [{i}] {status} {model.name}")
            print(f"       Ğ Ğ°Ğ·Ğ¼ĞµÑ€: {model.size_gb:.1f} GB | Ğ¢Ğ¸Ğ¿: {model.model_type}")
            if model.num_parameters:
                print(f"       ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹: ~{model.num_parameters}")
            print()
        
        print("   [0] âŒ Ğ’Ñ‹Ñ…Ğ¾Ğ´")
        print("=" * 70)
        
        try:
            choice = input("\n   Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ [0-{}]: ".format(len(models))).strip()
            
            if choice == "0" or choice == "":
                return None
            
            idx = int(choice) - 1
            if 0 <= idx < len(models):
                print(f"\n   âœ… Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ°: {models[idx].name}")
                return models[idx]
            else:
                print("   âŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€")
                return None
                
        except (ValueError, KeyboardInterrupt, EOFError):
            return None
    
    def show_datasets_menu(self, datasets: List[DatasetInfo]) -> Optional[DatasetInfo]:
        """Display datasets selection menu."""
        self._print_header("ğŸ“ ĞĞ‘ĞĞĞ Ğ£Ğ–Ğ•ĞĞĞ«Ğ• Ğ”ĞĞ¢ĞĞ¡Ğ•Ğ¢Ğ«")
        
        if not datasets:
            print()
            print("   âŒ Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹!")
            print()
            print(f"   ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ¿Ğ°Ğ¿ĞºÑƒ: {self.datasets_dir.absolute()}")
            print()
            print("   ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹:")
            print("   ./datasets/")
            print("   â””â”€â”€ train.jsonl")
            print()
            self.show_dataset_example()
            return None
        
        print(f"\n   ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²: {len(datasets)}\n")
        
        for i, ds in enumerate(datasets, 1):
            print(f"   [{i}] ğŸ“„ {ds.name}")
            print(f"       Ğ Ğ°Ğ·Ğ¼ĞµÑ€: {ds.size_mb:.1f} MB | Ğ¡Ñ‚Ñ€Ğ¾Ğº: {ds.num_lines:,} | Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: {ds.format}")
            print()
        
        print("   [E] ğŸ“ ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°")
        print("   [0] ğŸ”™ ĞĞ°Ğ·Ğ°Ğ´")
        print("=" * 70)
        
        try:
            choice = input("\n   Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ [0-{}/E]: ".format(len(datasets))).strip().upper()
            
            if choice == "E":
                self.show_dataset_example()
                return self.show_datasets_menu(datasets)  # Show menu again
            
            if choice == "0" or choice == "":
                return None
            
            idx = int(choice) - 1
            if 0 <= idx < len(datasets):
                selected = datasets[idx]
                
                # Validate dataset
                print(f"\n   â³ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°...")
                result = self.validate_dataset(selected.path)
                
                if result.valid:
                    print(f"   âœ… Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´ĞµĞ½: {result.valid_lines}/{result.total_lines} ÑÑ‚Ñ€Ğ¾Ğº OK")
                else:
                    print(f"   âš ï¸ ĞĞ°Ğ¹Ğ´ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹:")
                    for err in result.errors[:3]:
                        print(f"      - {err}")
                    
                    proceed = input("\n   ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ Ñ ÑÑ‚Ğ¸Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ¼? [y/N]: ").strip().lower()
                    if proceed not in ["y", "yes", "Ğ´Ğ°", "Ğ´"]:
                        return None
                
                return selected
            else:
                print("   âŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€")
                return None
                
        except (ValueError, KeyboardInterrupt, EOFError):
            return None
    
    def show_actions_menu(self, model: ModelInfo) -> Optional[str]:
        """Display actions menu for selected model."""
        self._print_header(f"ğŸ¯ Ğ”Ğ•Ğ™Ğ¡Ğ¢Ğ’Ğ˜Ğ¯: {model.name}")
        
        print()
        print("   Ğ§Ñ‚Ğ¾ Ğ²Ñ‹ Ñ…Ğ¾Ñ‚Ğ¸Ñ‚Ğµ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ?")
        print()
        print("   [1] ğŸ“Š ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (summary, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹)")
        print("   [2] ğŸ’¬ Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ñ‡Ğ°Ñ‚ (inference)")
        print("   [3] ğŸ¯ Fine-tune Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸")
        print()
        print("   [0] ğŸ”™ Ğ’Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ")
        print("=" * 70)
        
        try:
            choice = input("\n   Ğ’Ğ°Ñˆ Ğ²Ñ‹Ğ±Ğ¾Ñ€ [0-3]: ").strip()
            
            actions = {
                "1": "analyze",
                "2": "chat", 
                "3": "finetune",
                "0": None
            }
            
            return actions.get(choice, None)
            
        except (KeyboardInterrupt, EOFError):
            return None
    
    def run_analyze(self, model: ModelInfo):
        """Run model analysis."""
        self._print_header(f"ğŸ“Š ĞĞĞĞ›Ğ˜Ğ—: {model.name}")
        
        print()
        print("   â³ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°...")
        print()
        
        try:
            from .layer_utils import print_model_summary
            from transformers import AutoModelForCausalLM
            
            loaded_model = AutoModelForCausalLM.from_pretrained(
                model.path,
                trust_remote_code=True,
                device_map="auto"
            )
            
            print_model_summary(loaded_model)
            
            del loaded_model
            
        except Exception as e:
            print(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸: {e}")
        
        input("\n   ĞĞ°Ğ¶Ğ¼Ğ¸Ñ‚Ğµ Enter Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ...")
    
    def run_finetune_wizard(self, model: ModelInfo):
        """Run fine-tuning setup wizard with full validation."""
        # Disclaimer
        self._print_disclaimer(
            "Ğ”Ğ˜Ğ¡ĞšĞ›Ğ•Ğ™ĞœĞ•Ğ : FINE-TUNING",
            [
                "Fine-tuning Ğ¸Ğ·Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².",
                "",
                "Ğ¢Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ:",
                "â€¢ GPU Ñ VRAM >= 8GB (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ 24GB+)",
                "â€¢ ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ ChatML JSONL",
                "â€¢ Ğ¡Ğ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ½Ğ° Ğ´Ğ¸ÑĞºĞµ (Ğ´Ğ»Ñ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ¾Ğ²)",
                "â€¢ Ğ’Ñ€ĞµĞ¼Ñ: Ğ¾Ñ‚ 30 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ´Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‡Ğ°ÑĞ¾Ğ²",
                "",
                "ğŸ”¨ Transformers Forge Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚:",
                "â€¢ ProgressCallback â€” ĞºÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ±Ğ°Ñ€ Ñ ETA",
                "â€¢ EarlyStoppingCallback â€” Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (Y/N)",
                "â€¢ TrainingReportCallback â€” Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ training_report.md",
            ]
        )
        
        proceed = input("\n   ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ? [Y/n]: ").strip().lower()
        if proceed in ["n", "no", "Ğ½ĞµÑ‚", "Ğ½"]:
            return
        
        # Check dependencies first
        self._print_header("ğŸ” ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ Ğ—ĞĞ’Ğ˜Ğ¡Ğ˜ĞœĞĞ¡Ğ¢Ğ•Ğ™")
        
        missing_deps = []
        
        try:
            import trl
            print("   âœ… trl ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
        except ImportError:
            missing_deps.append("trl")
            print("   âŒ trl Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
        
        try:
            import datasets
            print("   âœ… datasets ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
        except ImportError:
            missing_deps.append("datasets")
            print("   âŒ datasets Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
        
        try:
            import peft
            print("   âœ… peft ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
        except ImportError:
            print("   âš ï¸ peft Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ (LoRA Ğ±ÑƒĞ´ĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½)")
        
        # Check GPU
        try:
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                print(f"   âœ… GPU: {gpu_name} ({vram:.1f} GB VRAM)")
            else:
                print("   âš ï¸ GPU Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ â€” Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ÑƒĞ´ĞµÑ‚ Ğ½Ğ° CPU (Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾)")
        except ImportError:
            print("   âš ï¸ PyTorch Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
            missing_deps.append("torch")
        
        if missing_deps:
            print()
            print(f"   âŒ ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸: {', '.join(missing_deps)}")
            print()
            print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
            print("   â”‚ ĞšĞĞš Ğ£Ğ¡Ğ¢ĞĞĞĞ’Ğ˜Ğ¢Ğ¬?                                                 â”‚")
            print("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
            print("   â”‚  [A] ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ (pip install Ğ¿Ñ€ÑĞ¼Ğ¾ ÑĞµĞ¹Ñ‡Ğ°Ñ)                   â”‚")
            print("   â”‚  [M] Ğ’Ñ€ÑƒÑ‡Ğ½ÑƒÑ (Ğ¿Ğ¾ĞºĞ°Ğ¶ĞµĞ¼ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ, Ğ²Ñ‹Ğ¹Ğ´ĞµÑ‚Ğµ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ ÑĞ°Ğ¼Ğ¸)       â”‚")
            print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            
            choice = input("\n   Ğ’Ğ°Ñˆ Ğ²Ñ‹Ğ±Ğ¾Ñ€ [A/M]: ").strip().upper()
            
            if choice == "A":
                print()
                print("   â³ Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸...")
                print(f"      pip install {' '.join(missing_deps)}")
                print()
                
                import subprocess
                result = subprocess.run(
                    ["pip", "install"] + missing_deps,
                    capture_output=False
                )
                
                if result.returncode == 0:
                    print()
                    print("   âœ… Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹!")
                    print("   ğŸ”„ ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ fine-tune Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ.")
                else:
                    print()
                    print("   âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ:")
                    print(f"      pip install {' '.join(missing_deps)}")
                
                input("\n   ĞĞ°Ğ¶Ğ¼Ğ¸Ñ‚Ğµ Enter Ğ´Ğ»Ñ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ°...")
                return
            else:
                print()
                print("   ğŸ“ Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚Ğµ Ğ² Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğµ:")
                print()
                print(f"      pip install {' '.join(missing_deps)}")
                print()
                print("   âš ï¸ Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ:")
                print("      1. ĞĞ°Ğ¶Ğ°Ñ‚ÑŒ Enter Ğ´Ğ»Ñ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ° Ğ² Ğ¼ĞµĞ½Ñ")
                print("      2. Ğ’Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ [0] Ğ´Ğ»Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°")
                print("      3. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğµ")
                print("      4. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ ÑĞ½Ğ¾Ğ²Ğ°")
                input("\n   ĞĞ°Ğ¶Ğ¼Ğ¸Ñ‚Ğµ Enter Ğ´Ğ»Ñ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ°...")
                return
        
        print()
        print("   âœ… Ğ’ÑĞµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ!")
        
        # Select dataset
        datasets = self.scan_datasets()
        dataset = self.show_datasets_menu(datasets)
        
        if dataset is None:
            return
        
        self.selected_dataset = dataset
        
        # Get training configuration with validation
        config = self._get_training_config_interactive()
        if config is None:
            return
        
        # Final confirmation
        self._print_header("âš™ï¸ Ğ˜Ğ¢ĞĞ“ĞĞ’ĞĞ¯ ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯")
        
        print()
        print(f"   ĞœĞ¾Ğ´ĞµĞ»ÑŒ:        {model.name}")
        print(f"   Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚:       {dataset.name} ({dataset.num_lines:,} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²)")
        print()
        print(f"   Learning Rate: {config['lr']}")
        print(f"   Batch Size:    {config['batch_size']}")
        print(f"   Epochs:        {config['epochs']}")
        print(f"   LoRA:          {'Ğ”Ğ°' if config['use_lora'] else 'ĞĞµÑ‚'}")
        print(f"   Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ:    {config['output_dir']}")
        print()
        
        # Show Forge technologies info
        print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print("   â”‚ ğŸ”¨ Ğ¢Ğ•Ğ¥ĞĞĞ›ĞĞ“Ğ˜Ğ˜ TRANSFORMERS FORGE                                â”‚")
        print("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
        print("   â”‚  âœ… ProgressCallback â€” ĞºÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ±Ğ°Ñ€ Ñ ETA              â”‚")
        print("   â”‚  âœ… EarlyStoppingCallback â€” Ğ°Ğ²Ñ‚Ğ¾-ÑÑ‚Ğ¾Ğ¿ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸          â”‚")
        print("   â”‚     (Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¿Ñ€Ğ¾ÑĞ¸Ñ‚ Y/N Ğ¿ĞµÑ€ĞµĞ´ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¾Ğ¹)                 â”‚")
        print("   â”‚  âœ… TrainingReportCallback â€” Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ               â”‚")
        print("   â”‚     (ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ training_report.md Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸)                 â”‚")
        print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        print()
        
        confirm = input("   ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ? [Y/n]: ").strip().lower()
        if confirm in ["n", "no", "Ğ½ĞµÑ‚", "Ğ½"]:
            print("   âŒ ĞÑ‚Ğ¼ĞµĞ½ĞµĞ½Ğ¾")
            return
        
        # Run training!
        self._run_finetune_training(model, dataset, config)
    
    def _get_training_config_interactive(self) -> Optional[Dict[str, Any]]:
        """Get training configuration with validation, presets and Auto mode."""
        self._print_header("âš™ï¸ ĞĞĞ¡Ğ¢Ğ ĞĞ™ĞšĞ ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ ĞĞ’")
        
        print()
        print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print("   â”‚ ğŸ’¡ ĞŸĞĞ”Ğ¡ĞšĞĞ—ĞšĞ                                                    â”‚")
        print("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
        print("   â”‚  â€¢ Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ 'Auto' â€” Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸   â”‚")
        print("   â”‚  â€¢ Ğ˜Ğ»Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ğ¹Ñ‚Ğµ Ğ¿Ñ€ĞµÑĞµÑ‚Ñ‹ A/B/C Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°           â”‚")
        print("   â”‚  â€¢ B = Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ (â­)                                     â”‚")
        print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        print()
        
        # Check for Auto mode
        auto_check = input("   Ğ ĞµĞ¶Ğ¸Ğ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ [Auto/manual]: ").strip().lower()
        
        if auto_check in ["auto", "Ğ°", "Ğ°Ğ²Ñ‚Ğ¾", ""]:
            print()
            print("   âœ… Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½ Ñ€ĞµĞ¶Ğ¸Ğ¼ Auto â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸:")
            print("      â€¢ Learning Rate: 2e-5")
            print("      â€¢ Batch Size: 4")
            print("      â€¢ Epochs: 3")
            print("      â€¢ LoRA: Ğ”Ğ°")
            print("      â€¢ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ²: ./output")
            return {
                "lr": 2e-5,
                "batch_size": 4,
                "epochs": 3,
                "use_lora": True,
                "output_dir": "./output"
            }
        
        print()
        print("   ğŸ“ Ğ ÑƒÑ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²")
        print()
        
        # Learning Rate
        lr = self._get_validated_lr_with_presets()
        if lr is None:
            return None
        
        # Batch Size
        batch_size = self._get_validated_batch_size_with_presets()
        if batch_size is None:
            return None
        
        # Epochs
        epochs = self._get_validated_epochs_with_presets()
        if epochs is None:
            return None
        
        # LoRA
        print()
        print("   LoRA (ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ):")
        print("      [A] ĞĞµÑ‚ â€” Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ VRAM)")
        print("      [B] Ğ”Ğ°  â€” LoRA Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ (â­ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ)")
        use_lora_input = input("   Ğ’Ğ°Ñˆ Ğ²Ñ‹Ğ±Ğ¾Ñ€ [A/B]: ").strip().upper()
        use_lora = use_lora_input != "A"
        
        # Output directory
        print()
        print("   ĞšÑƒĞ´Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ?")
        output_dir = input("   ĞŸÑƒÑ‚ÑŒ [./output]: ").strip()
        if not output_dir:
            output_dir = "./output"
        print(f"   âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±ÑƒĞ´ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° Ğ²: {output_dir}")
        
        return {
            "lr": lr,
            "batch_size": batch_size,
            "epochs": epochs,
            "use_lora": use_lora,
            "output_dir": output_dir
        }
    
    def _get_validated_lr_with_presets(self) -> Optional[float]:
        """Get learning rate with A/B/C presets."""
        print("   Learning Rate:")
        print("      [A] 5e-5  â€” Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ (Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ)")
        print("      [B] 2e-5  â€” Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ (â­)")
        print("      [C] 1e-5  â€” ĞºĞ¾Ğ½ÑĞµÑ€Ğ²Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ (Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ)")
        print("      Ğ˜Ğ»Ğ¸ Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ ÑĞ²Ğ¾Ñ‘ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€: 3e-5)")
        
        presets = {"a": 5e-5, "b": 2e-5, "c": 1e-5}
        
        max_attempts = 3
        for attempt in range(max_attempts):
            choice = input("   Ğ’Ğ°Ñˆ Ğ²Ñ‹Ğ±Ğ¾Ñ€ [A/B/C Ğ¸Ğ»Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾]: ").strip().lower()
            
            if choice in presets:
                lr = presets[choice]
                label = "Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹" if choice == "a" else "Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹" if choice == "b" else "ĞºĞ¾Ğ½ÑĞµÑ€Ğ²Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹"
                print(f"   âœ… Learning Rate: {lr} ({label})")
                return lr
            
            # Try to parse as number
            try:
                lr = float(choice)
                
                if lr <= 0:
                    print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Learning rate Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ > 0")
                    continue
                
                if lr > 1e-2:
                    print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Learning rate ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ (Ğ¼Ğ°ĞºÑ. 1e-2)")
                    print("      Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ»Ğ¸ÑÑŒ? Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ¿Ñ€ĞµÑĞµÑ‚Ñ‹ A/B/C")
                    continue
                
                # Warning for unusual values
                if lr > 1e-3:
                    print(f"   âš ï¸ Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ LR ({lr}) â€” Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾")
                    confirm = input("   ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ? [y/N]: ").strip().lower()
                    if confirm not in ["y", "yes", "Ğ´Ğ°", "Ğ´"]:
                        continue
                
                print(f"   âœ… Learning Rate: {lr} (ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¹)")
                return lr
                
            except ValueError:
                print("   âŒ Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ A, B, C Ğ¸Ğ»Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€: 2e-5)")
        
        print("   âŒ ĞŸÑ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº")
        return None
    
    def _get_validated_batch_size_with_presets(self) -> Optional[int]:
        """Get batch size with A/B/C presets."""
        print()
        print("   Batch Size:")
        print("      [A] 8   â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞµ (Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ VRAM)")
        print("      [B] 4   â€” Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ (â­)")
        print("      [C] 2   â€” Ğ¼ĞµĞ½ÑŒÑˆĞµ (ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ)")
        print("      Ğ˜Ğ»Ğ¸ Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ ÑĞ²Ğ¾Ñ‘ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ")
        
        presets = {"a": 8, "b": 4, "c": 2}
        
        max_attempts = 3
        for attempt in range(max_attempts):
            choice = input("   Ğ’Ğ°Ñˆ Ğ²Ñ‹Ğ±Ğ¾Ñ€ [A/B/C Ğ¸Ğ»Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾]: ").strip().lower()
            
            if choice in presets:
                bs = presets[choice]
                label = "Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹" if choice == "a" else "Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹" if choice == "b" else "ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹"
                print(f"   âœ… Batch Size: {bs} ({label})")
                return bs
            
            try:
                bs = int(choice)
                
                if bs <= 0:
                    print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Batch size Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ > 0")
                    continue
                
                if bs > 64:
                    print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Batch size ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ (Ğ¼Ğ°ĞºÑ. 64)")
                    print("      Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ¿Ñ€ĞµÑĞµÑ‚Ñ‹ A/B/C")
                    continue
                
                if bs > 16:
                    print(f"   âš ï¸ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ batch ({bs}) â€” Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ VRAM")
                    confirm = input("   ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ? [y/N]: ").strip().lower()
                    if confirm not in ["y", "yes", "Ğ´Ğ°", "Ğ´"]:
                        continue
                
                print(f"   âœ… Batch Size: {bs} (ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¹)")
                return bs
                
            except ValueError:
                print("   âŒ Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ A, B, C Ğ¸Ğ»Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾")
        
        print("   âŒ ĞŸÑ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº")
        return None
    
    def _get_validated_epochs_with_presets(self) -> Optional[int]:
        """Get epochs with A/B/C presets."""
        print()
        print("   Epochs (ÑĞ¿Ğ¾Ñ…Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ):")
        print("      [A] 5   â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞµ (Ğ´Ğ¾Ğ»ÑŒÑˆĞµ, Ñ€Ğ¸ÑĞº Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ)")
        print("      [B] 3   â€” Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ (â­)")
        print("      [C] 1   â€” Ğ¼ĞµĞ½ÑŒÑˆĞµ (Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾, Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²)")
        print("      Ğ˜Ğ»Ğ¸ Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ ÑĞ²Ğ¾Ñ‘ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ")
        
        presets = {"a": 5, "b": 3, "c": 1}
        
        max_attempts = 3
        for attempt in range(max_attempts):
            choice = input("   Ğ’Ğ°Ñˆ Ğ²Ñ‹Ğ±Ğ¾Ñ€ [A/B/C Ğ¸Ğ»Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾]: ").strip().lower()
            
            if choice in presets:
                ep = presets[choice]
                label = "Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ğ¹" if choice == "a" else "Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹" if choice == "b" else "Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹"
                print(f"   âœ… Epochs: {ep} ({label})")
                return ep
            
            try:
                ep = int(choice)
                
                if ep <= 0:
                    print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Epochs Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ > 0")
                    continue
                
                if ep > 20:
                    print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ¾Ñ… (Ğ¼Ğ°ĞºÑ. 20)")
                    print("      Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ¿Ñ€ĞµÑĞµÑ‚Ñ‹ A/B/C")
                    continue
                
                if ep > 10:
                    print(f"   âš ï¸ ĞœĞ½Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ¾Ñ… ({ep}) â€” Ñ€Ğ¸ÑĞº Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ")
                    confirm = input("   ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ? [y/N]: ").strip().lower()
                    if confirm not in ["y", "yes", "Ğ´Ğ°", "Ğ´"]:
                        continue
                
                print(f"   âœ… Epochs: {ep} (ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¹)")
                return ep
                
            except ValueError:
                print("   âŒ Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ A, B, C Ğ¸Ğ»Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾")
        
        print("   âŒ ĞŸÑ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº")
        return None
    
    def _get_validated_lr(self) -> Optional[float]:
        """Get and validate learning rate with disclaimers."""
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                lr_input = input("   Learning Rate [2e-5]: ").strip()
                
                if lr_input == "":
                    return 2e-5
                
                lr = float(lr_input)
                
                # Validate
                if lr <= 0:
                    print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Learning rate Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ > 0")
                    continue
                
                if lr > 1e-2:
                    print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Learning rate ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ (Ğ¼Ğ°ĞºÑ. 1e-2)")
                    print("      Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ»Ğ¸ÑÑŒ Ğ¿Ñ€Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğµ?")
                    continue
                
                # Critical values - need confirmation
                if lr > 1e-3:
                    if not self._confirm_critical_setting(
                        setting="Learning Rate",
                        value=str(lr),
                        issue="Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ LR (> 1e-3) Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                        recommendation="Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ: 1e-5 Ğ´Ğ¾ 5e-5",
                        explanation=[
                            "Learning rate Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑˆĞ°Ğ³Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ².",
                            "Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ LR Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº:",
                            "  â€¢ Ğ¥Ğ°Ğ¾Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ Ğ²ĞµÑĞ¾Ğ²",
                            "  â€¢ Loss Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ",
                            "  â€¢ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ 'Ñ€Ğ°Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ' Ğ¸ Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼ÑƒÑĞ¾Ñ€",
                            "",
                            "Ğ•ÑĞ»Ğ¸ Ğ²Ñ‹ ÑƒĞ²ĞµÑ€ĞµĞ½Ñ‹ Ğ¸ Ñ…Ğ¾Ñ‚Ğ¸Ñ‚Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ â€” Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°Ğ¹Ñ‚Ğµ.",
                        ]
                    ):
                        continue
                
                if lr < 1e-7:
                    if not self._confirm_critical_setting(
                        setting="Learning Rate",
                        value=str(lr),
                        issue="ĞÑ‡ĞµĞ½ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¹ LR (< 1e-7) â€” Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ÑƒĞ´ĞµÑ‚ ĞºÑ€Ğ°Ğ¹Ğ½Ğµ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼",
                        recommendation="Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ: 1e-5 Ğ´Ğ¾ 5e-5",
                        explanation=[
                            "Ğ¡ Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ learning rate Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½Ğµ Ğ±ÑƒĞ´ĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ.",
                            "ĞŸĞ¾Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ² 100+ Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑĞ¿Ğ¾Ñ… Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°.",
                            "",
                            "Ğ•ÑĞ»Ğ¸ ÑÑ‚Ğ¾ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ â€” Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°Ğ¹Ñ‚Ğµ.",
                        ]
                    ):
                        continue
                
                return lr
                
            except ValueError:
                print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Learning rate Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼")
                print("      ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: 2e-5, 0.00002, 5e-6")
        
        print(f"   âŒ ĞŸÑ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº")
        return None
    
    def _get_validated_batch_size(self) -> Optional[int]:
        """Get and validate batch size."""
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                bs_input = input("   Batch Size [4]: ").strip()
                
                if bs_input == "":
                    return 4
                
                batch_size = int(bs_input)
                
                if batch_size <= 0:
                    print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Batch size Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ > 0")
                    continue
                
                if batch_size > 128:
                    print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Batch size ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ (Ğ¼Ğ°ĞºÑ. 128)")
                    print("      Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ»Ğ¸ÑÑŒ Ğ¿Ñ€Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğµ?")
                    continue
                
                # Critical - large batch
                if batch_size > 32:
                    if not self._confirm_critical_setting(
                        setting="Batch Size",
                        value=str(batch_size),
                        issue="Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ batch size Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ VRAM",
                        recommendation="Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ: 4-16 Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° GPU",
                        explanation=[
                            f"Batch size {batch_size} Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ 40+ GB VRAM.",
                            "Ğ•ÑĞ»Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚ â€” Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑƒĞ¿Ğ°Ğ´Ñ‘Ñ‚ Ñ OOM Ğ¾ÑˆĞ¸Ğ±ĞºĞ¾Ğ¹.",
                            "",
                            "Ğ¡Ğ¾Ğ²ĞµÑ‚: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ gradient_accumulation_steps Ğ²Ğ¼ĞµÑÑ‚Ğ¾",
                            "Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ batch_size Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ñ‚Ñ‡Ğ°.",
                        ]
                    ):
                        continue
                
                return batch_size
                
            except ValueError:
                print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Batch size Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ñ†ĞµĞ»Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼")
                print("      ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: 4, 8, 16")
        
        print(f"   âŒ ĞŸÑ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº")
        return None
    
    def _get_validated_epochs(self) -> Optional[int]:
        """Get and validate number of epochs."""
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                ep_input = input("   Epochs [3]: ").strip()
                
                if ep_input == "":
                    return 3
                
                epochs = int(ep_input)
                
                if epochs <= 0:
                    print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Epochs Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ > 0")
                    continue
                
                if epochs > 100:
                    print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ¾Ñ… (Ğ¼Ğ°ĞºÑ. 100)")
                    print("      Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ»Ğ¸ÑÑŒ Ğ¿Ñ€Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğµ?")
                    continue
                
                # Critical - many epochs
                if epochs > 10:
                    if not self._confirm_critical_setting(
                        setting="Epochs",
                        value=str(epochs),
                        issue="ĞœĞ½Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ¾Ñ… â€” Ñ€Ğ¸ÑĞº Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                        recommendation="Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ: 1-5 Ğ´Ğ»Ñ fine-tuning",
                        explanation=[
                            f"{epochs} ÑĞ¿Ğ¾Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (overfitting).",
                            "ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ½Ğ°Ğ¸Ğ·ÑƒÑÑ‚ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ.",
                            "ĞĞ±Ñ‹Ñ‡Ğ½Ğ¾ 1-3 ÑĞ¿Ğ¾Ñ…Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ fine-tuning.",
                            "",
                            "Ğ•ÑĞ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ (100k+ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²) â€”",
                            "Ğ´Ğ°Ğ¶Ğµ 1 ÑĞ¿Ğ¾Ñ…Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾.",
                        ]
                    ):
                        continue
                
                return epochs
                
            except ValueError:
                print("   âŒ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾: Epochs Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ñ†ĞµĞ»Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼")
                print("      ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: 1, 2, 3")
        
        print(f"   âŒ ĞŸÑ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº")
        return None
    
    def _confirm_critical_setting(
        self,
        setting: str,
        value: str,
        issue: str,
        recommendation: str,
        explanation: List[str]
    ) -> bool:
        """
        Two-stage confirmation for critical settings.
        
        First asks Y/n, if N shows detailed explanation and asks again.
        """
        # First confirmation
        print()
        print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print(f"   â”‚ âš ï¸  Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•: Ğ¡Ğ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸                           â”‚")
        print("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
        print(f"   â”‚  {setting}: {value}")
        print(f"   â”‚  ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: {issue}")
        print(f"   â”‚  {recommendation}")
        print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        
        first = input("\n   ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ Ñ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ğ¼Ğ¸? [y/N]: ").strip().lower()
        
        if first in ["y", "yes", "Ğ´Ğ°", "Ğ´"]:
            return True
        
        # Second confirmation with detailed explanation
        print()
        print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print(f"   â”‚ ğŸ“– ĞŸĞĞ”Ğ ĞĞ‘ĞĞĞ• ĞĞ‘ĞªĞ¯Ğ¡ĞĞ•ĞĞ˜Ğ•                                         â”‚")
        print("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
        for line in explanation:
            print(f"   â”‚  {line}")
        print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        
        second = input("\n   Ğ’Ñ‹ ÑƒĞ²ĞµÑ€ĞµĞ½Ñ‹? Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸? [y/N]: ").strip().lower()
        
        return second in ["y", "yes", "Ğ´Ğ°", "Ğ´"]
    
    def _run_finetune_training(self, model: ModelInfo, dataset: DatasetInfo, config: Dict[str, Any]):
        """Actually run the fine-tuning process."""
        self._print_header("ğŸ”¥ Ğ—ĞĞŸĞ£Ğ¡Ğš ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ¯")
        
        print()
        print("   â³ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°...")
        
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
            from trl import SFTTrainer, SFTConfig
            from datasets import load_dataset
            
            # Import our callbacks
            from .training_monitor import (
                ProgressCallback,
                EarlyStoppingCallback,
                TrainingReportCallback
            )
            
            # Create output directory if needed
            output_dir = Path(config["output_dir"])
            output_dir.mkdir(parents=True, exist_ok=True)
            print(f"   ğŸ“ ĞŸĞ°Ğ¿ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ: {output_dir.absolute()}")
            
            # Load model
            print(f"   ğŸ“¦ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° {model.name}...")
            
            load_kwargs = {
                "trust_remote_code": True,
            }
            
            # Try to use GPU efficiently
            try:
                import torch
                if torch.cuda.is_available():
                    load_kwargs["device_map"] = "auto"
                    load_kwargs["torch_dtype"] = torch.bfloat16
                    print("   âœ… GPU Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ bf16")
            except:
                pass
            
            loaded_model = AutoModelForCausalLM.from_pretrained(
                model.path,
                **load_kwargs
            )
            
            tokenizer = AutoTokenizer.from_pretrained(
                model.path,
                trust_remote_code=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            print("   âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°!")
            
            # Load dataset
            print(f"   ğŸ“ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° {dataset.name}...")
            train_dataset = load_dataset("json", data_files=dataset.path, split="train")
            print(f"   âœ… Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½: {len(train_dataset)} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²")
            
            # Setup LoRA if enabled
            if config["use_lora"]:
                try:
                    from peft import LoraConfig, get_peft_model
                    
                    print("   ğŸ”§ ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ LoRA...")
                    
                    lora_config = LoraConfig(
                        r=16,
                        lora_alpha=32,
                        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
                        lora_dropout=0.05,
                        bias="none",
                        task_type="CAUSAL_LM"
                    )
                    
                    loaded_model = get_peft_model(loaded_model, lora_config)
                    loaded_model.print_trainable_parameters()
                    
                except ImportError:
                    print("   âš ï¸ PEFT Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½, LoRA Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½")
                    print("      Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: pip install peft")
            
            # Training arguments
            print("   âš™ï¸ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ...")
            
            training_args = SFTConfig(
                output_dir=config["output_dir"],
                num_train_epochs=config["epochs"],
                per_device_train_batch_size=config["batch_size"],
                learning_rate=config["lr"],
                logging_steps=10,
                save_steps=500,
                save_total_limit=2,
                warmup_ratio=0.1,
                gradient_accumulation_steps=4,
                fp16=False,
                bf16=True,
                max_seq_length=2048,
                packing=False,
            )
            
            # Create trainer with our callbacks
            report_path = f"{config['output_dir']}/training_report.md"
            trainer = SFTTrainer(
                model=loaded_model,
                train_dataset=train_dataset,
                tokenizer=tokenizer,
                args=training_args,
                callbacks=[
                    ProgressCallback(),
                    EarlyStoppingCallback(patience=3, interactive=True),
                    TrainingReportCallback(output_path=report_path, interactive=False)
                ]
            )
            
            print()
            print("   ğŸš€ ĞĞĞ§Ğ˜ĞĞĞ•Ğœ ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ•!")
            print("=" * 70)
            print()
            
            # Train!
            trainer.train()
            
            print()
            print("=" * 70)
            print("   âœ… ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ• Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ!")
            print()
            print(f"   ğŸ“ ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ°: {config['output_dir']}")
            print(f"   ğŸ“„ ĞÑ‚Ñ‡Ñ‘Ñ‚: {config['output_dir']}/training_report.md")
            print("=" * 70)
            
        except ImportError as e:
            print(f"\n   âŒ ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸: {e}")
            print("      Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: pip install trl datasets peft")
            
        except Exception as e:
            print(f"\n   âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: {e}")
            import traceback
            traceback.print_exc()
        
        input("\n   ĞĞ°Ğ¶Ğ¼Ğ¸Ñ‚Ğµ Enter Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ...")
    
    def run(self):
        """Main interactive loop."""
        print()
        print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘  ğŸ”¨ TRANSFORMERS FORGE â€” Interactive Model Manager                   â•‘")
        print("â•‘  v1.0.9                                                              â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print()
        print(f"   ğŸ“‚ ĞŸĞ°Ğ¿ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹:   {self.models_dir.absolute()}")
        print(f"   ğŸ“ ĞŸĞ°Ğ¿ĞºĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²: {self.datasets_dir.absolute()}")
        
        while True:
            # Scan models
            models = self.scan_models()
            
            # Select model
            model = self.show_models_menu(models)
            if model is None:
                print("\n   ğŸ‘‹ Ğ”Ğ¾ ÑĞ²Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ!")
                break
            
            self.selected_model = model
            
            # Actions loop
            while True:
                action = self.show_actions_menu(model)
                
                if action is None:
                    break
                elif action == "analyze":
                    self.run_analyze(model)
                elif action == "chat":
                    print("\n   ğŸ’¬ Ğ§Ğ°Ñ‚ Ğ±ÑƒĞ´ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ² ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸...")
                    input("   ĞĞ°Ğ¶Ğ¼Ğ¸Ñ‚Ğµ Enter...")
                elif action == "finetune":
                    self.run_finetune_wizard(model)


# =============================================================================
# Entry point
# =============================================================================


def interactive_start(models_dir: str = "./models", datasets_dir: str = "./datasets"):
    """
    Start the interactive model manager.
    
    Args:
        models_dir: Path to models directory
        datasets_dir: Path to datasets directory
        
    Example:
        >>> from transformers.interactive import interactive_start
        >>> interactive_start("./models", "./datasets")
    """
    manager = InteractiveModelManager(
        models_dir=models_dir,
        datasets_dir=datasets_dir
    )
    manager.run()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Transformers Forge Interactive Manager")
    parser.add_argument("--models", default="./models", help="Path to models directory")
    parser.add_argument("--datasets", default="./datasets", help="Path to datasets directory")
    
    args = parser.parse_args()
    
    interactive_start(args.models, args.datasets)
