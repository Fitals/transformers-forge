# Layer Utils

**–ú–æ–¥—É–ª—å:** `transformers.layer_utils`

–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ—è–º–∏ –º–æ–¥–µ–ª–∏: –∑–∞–º–æ—Ä–æ–∑–∫–∞, —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∞, –∞–Ω–∞–ª–∏–∑.

---

## üìä –ó–∞—á–µ–º –Ω—É–∂–µ–Ω Layer Utils?

| –ü—Ä–æ–±–ª–µ–º–∞ | –†–µ—à–µ–Ω–∏–µ |
|----------|---------|
| –ü–∞–º—è—Ç–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –Ω–∞ –≤—Å–µ —Å–ª–æ–∏ | –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å —á–∞—Å—Ç—å —Å–ª–æ—ë–≤ |
| LP-LoRA —Ç—Ä–µ–±—É–µ—Ç –∑–∞–º–æ—Ä–æ–∑–∫—É | `freeze_first_n_layers()` |
| –ù—É–∂–Ω–æ –ø–æ–Ω—è—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ | `print_layer_status()` |

**–≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏:** –¥–æ 50%+ –ø—Ä–∏ –∑–∞–º–æ—Ä–æ–∑–∫–µ –ø–æ–ª–æ–≤–∏–Ω—ã —Å–ª–æ—ë–≤

---

## üîß –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```python
from transformers import AutoModelForCausalLM
from transformers import freeze_first_n_layers, get_frozen_percentage

model = AutoModelForCausalLM.from_pretrained("model_name")

# –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å –ø–µ—Ä–≤—ã–µ 16 —Å–ª–æ—ë–≤ (LP-LoRA —Å—Ç–∏–ª—å)
freeze_first_n_layers(model, n=16)

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å
print(f"Frozen: {get_frozen_percentage(model):.1f}%")
```

---

## üìñ API Reference

### –§—É–Ω–∫—Ü–∏–∏ –∑–∞–º–æ—Ä–æ–∑–∫–∏

#### freeze_model

```python
def freeze_model(model) -> int
```

–ó–∞–º–æ—Ä–æ–∑–∏—Ç—å –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏.

```python
from transformers import freeze_model

frozen_count = freeze_model(model)
print(f"Frozen {frozen_count:,} parameters")
```

---

#### freeze_first_n_layers

```python
def freeze_first_n_layers(model, n: int) -> int
```

–ó–∞–º–æ—Ä–æ–∑–∏—Ç—å –ø–µ—Ä–≤—ã–µ N —Å–ª–æ—ë–≤. **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è LP-LoRA.**

```python
from transformers import freeze_first_n_layers

# –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å –ø–µ—Ä–≤—ã–µ 16 —Å–ª–æ—ë–≤
freeze_first_n_layers(model, 16)
```

---

#### freeze_last_n_layers

```python
def freeze_last_n_layers(model, n: int) -> int
```

–ó–∞–º–æ—Ä–æ–∑–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–ª–æ—ë–≤.

---

#### freeze_except_last_n

```python
def freeze_except_last_n(model, n: int) -> int
```

–ó–∞–º–æ—Ä–æ–∑–∏—Ç—å –≤—Å—ë –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —Å–ª–æ—ë–≤.

```python
from transformers import freeze_except_last_n

# –û–±—É—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 —Å–ª–æ—è
freeze_except_last_n(model, n=4)
```

---

#### freeze_embeddings

```python
def freeze_embeddings(model) -> int
```

–ó–∞–º–æ—Ä–æ–∑–∏—Ç—å —Å–ª–æ–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç "–∑–∞–±—ã–≤–∞–Ω–∏–µ" –∑–Ω–∞–Ω–∏–π.

```python
from transformers import freeze_embeddings

freeze_embeddings(model)
```

---

#### freeze_by_name

```python
def freeze_by_name(
    model,
    patterns: Union[str, List[str]],
    case_sensitive: bool = True
) -> int
```

–ó–∞–º–æ—Ä–æ–∑–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ regex –ø–∞—Ç—Ç–µ—Ä–Ω—É.

```python
from transformers import freeze_by_name

# –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å –≤—Å–µ attention —Å–ª–æ–∏
freeze_by_name(model, [".*attention.*", ".*attn.*"])
```

---

### –§—É–Ω–∫—Ü–∏–∏ —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∏

#### unfreeze_model

```python
def unfreeze_model(model) -> int
```

–†–∞–∑–º–æ—Ä–æ–∑–∏—Ç—å –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.

---

### –§—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞

#### get_trainable_params

```python
def get_trainable_params(model) -> int
```

–ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.

```python
from transformers import get_trainable_params

trainable = get_trainable_params(model)
print(f"Trainable: {trainable:,}")
```

---

#### get_frozen_percentage

```python
def get_frozen_percentage(model) -> float
```

–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ—Ü–µ–Ω—Ç –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

```python
from transformers import get_frozen_percentage

pct = get_frozen_percentage(model)
print(f"Frozen: {pct:.1f}%")
```

---

#### get_num_layers

```python
def get_num_layers(model) -> int
```

–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ –≤ –º–æ–¥–µ–ª–∏.

```python
from transformers import get_num_layers

num = get_num_layers(model)
print(f"Model has {num} layers")
```

---

#### print_layer_status

```python
def print_layer_status(model, show_all: bool = False)
```

–í—ã–≤–µ—Å—Ç–∏ —Ç–∞–±–ª–∏—Ü—É —Å—Ç–∞—Ç—É—Å–∞ —Å–ª–æ—ë–≤.

```python
from transformers import print_layer_status

print_layer_status(model)
# ========================================
# LAYER STATUS
# ========================================
# Layer 0-15:  FROZEN  (1.2B params)
# Layer 16-31: ACTIVE  (1.2B params)
# ----------------------------------------
# Embeddings:  FROZEN
# LM Head:     ACTIVE
# ========================================
```

---

### GradualUnfreezer

```python
class GradualUnfreezer:
    def __init__(
        self,
        model,
        total_epochs: int,
        unfreeze_embeddings_at: int = None,
        freeze_embeddings: bool = True,
        verbose: bool = True
    )
```

–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∞ —Å–ª–æ—ë–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ transfer learning.

```python
from transformers import GradualUnfreezer

unfreezer = GradualUnfreezer(model, total_epochs=10)

for epoch in range(10):
    unfreezer.step(epoch)  # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç —Å–ª–æ–∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ
    train_one_epoch(model, ...)
```

---

### setup_lp_lora_style

```python
def setup_lp_lora_style(
    model,
    freeze_ratio: float = 0.5,
    freeze_embed: bool = True
) -> int
```

–ë—ã—Å—Ç—Ä–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LP-LoRA —Å—Ç–∏–ª—è.

```python
from transformers import setup_lp_lora_style

# –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å 50% —Å–ª–æ—ë–≤ + —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
setup_lp_lora_style(model, freeze_ratio=0.5)
```

---

### get_memory_savings_estimate

```python
def get_memory_savings_estimate(model) -> Dict[str, float]
```

–û—Ü–µ–Ω–∏—Ç—å —ç–∫–æ–Ω–æ–º–∏—é –ø–∞–º—è—Ç–∏ –æ—Ç –∑–∞–º–æ—Ä–æ–∑–∫–∏.

```python
from transformers import get_memory_savings_estimate

savings = get_memory_savings_estimate(model)
print(f"Gradient memory saved: {savings['gradient_saved_gb']:.2f} GB")
print(f"Optimizer memory saved: {savings['optimizer_saved_gb']:.2f} GB")
```

---

## üéØ –¢–∏–ø–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏

### LP-LoRA —Å—Ç–∏–ª—å

```python
from transformers import setup_lp_lora_style

setup_lp_lora_style(model, freeze_ratio=0.5)
# –†–µ–∑—É–ª—å—Ç–∞—Ç: –ø–µ—Ä–≤—ã–µ 50% —Å–ª–æ—ë–≤ –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã
```

### –û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ—ë–≤

```python
from transformers import freeze_except_last_n

freeze_except_last_n(model, n=4)
# –†–µ–∑—É–ª—å—Ç–∞—Ç: —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 —Å–ª–æ—è –æ–±—É—á–∞—é—Ç—Å—è
```

### –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∞

```python
from transformers import GradualUnfreezer

unfreezer = GradualUnfreezer(model, total_epochs=10)
# –≠–ø–æ—Ö–∞ 1: —Ç–æ–ª—å–∫–æ lm_head
# –≠–ø–æ—Ö–∞ 5: –ø–æ–ª–æ–≤–∏–Ω–∞ —Å–ª–æ—ë–≤
# –≠–ø–æ—Ö–∞ 10: –≤—Å–µ —Å–ª–æ–∏
```

---

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

1. **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:** –í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω—è—é—Ç `requires_grad`, –Ω–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—é—Ç –≤–µ—Å–∞
2. **–û–±—Ä–∞—Ç–∏–º–æ—Å—Ç—å:** –ú–æ–∂–Ω–æ –≤ –ª—é–±–æ–π –º–æ–º–µ–Ω—Ç –≤—ã–∑–≤–∞—Ç—å `unfreeze_model()`
3. **–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å:** –†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±–æ–π PyTorch –º–æ–¥–µ–ª—å—é
