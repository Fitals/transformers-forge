# Layer Utils

**ĞœĞ¾Ğ´ÑƒĞ»ÑŒ:** `transformers.layer_utils`

Ğ£Ñ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ°, Ñ€Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ°, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·.

---

## ğŸ“Š Ğ—Ğ°Ñ‡ĞµĞ¼ Ğ½ÑƒĞ¶ĞµĞ½ Layer Utils?

| ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° | Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ |
|----------|---------|
| ĞŸĞ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ²ÑĞµ ÑĞ»Ğ¾Ğ¸ | Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ñ‡Ğ°ÑÑ‚ÑŒ ÑĞ»Ğ¾Ñ‘Ğ² |
| LP-LoRA Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºÑƒ | `freeze_first_n_layers()` |
| ĞÑƒĞ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ | `print_layer_status()` |

**Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸:** Ğ´Ğ¾ 50%+ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞµ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ñ‹ ÑĞ»Ğ¾Ñ‘Ğ²

---

## ğŸ”§ Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚

```python
from transformers import AutoModelForCausalLM
from transformers import freeze_first_n_layers, get_frozen_percentage

model = AutoModelForCausalLM.from_pretrained("model_name")

# Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 16 ÑĞ»Ğ¾Ñ‘Ğ² (LP-LoRA ÑÑ‚Ğ¸Ğ»ÑŒ)
freeze_first_n_layers(model, n=16)

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ
print(f"Frozen: {get_frozen_percentage(model):.1f}%")
```

---

## ğŸ“– API Reference

### Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ¸

#### freeze_model

```python
def freeze_model(model) -> int
```

Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.

```python
from transformers import freeze_model

frozen_count = freeze_model(model)
print(f"Frozen {frozen_count:,} parameters")
```

---

#### freeze_first_n_layers

```python
def freeze_first_n_layers(model, n: int) -> int
```

Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ N ÑĞ»Ğ¾Ñ‘Ğ². **Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ LP-LoRA.**

```python
from transformers import freeze_first_n_layers

# Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 16 ÑĞ»Ğ¾Ñ‘Ğ²
freeze_first_n_layers(model, 16)
```

---

#### freeze_last_n_layers

```python
def freeze_last_n_layers(model, n: int) -> int
```

Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ N ÑĞ»Ğ¾Ñ‘Ğ².

---

#### freeze_except_last_n

```python
def freeze_except_last_n(model, n: int) -> int
```

Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ²ÑÑ‘ ĞºÑ€Ğ¾Ğ¼Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… N ÑĞ»Ğ¾Ñ‘Ğ².

```python
from transformers import freeze_except_last_n

# ĞĞ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 4 ÑĞ»Ğ¾Ñ
freeze_except_last_n(model, n=4)
```

---

#### freeze_embeddings

```python
def freeze_embeddings(model) -> int
```

Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ "Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ" Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.

```python
from transformers import freeze_embeddings

freeze_embeddings(model)
```

---

#### freeze_by_name

```python
def freeze_by_name(
    model,
    patterns: Union[str, List[str]],
    case_sensitive: bool = True
) -> int
```

Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ¾ regex Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñƒ.

```python
from transformers import freeze_by_name

# Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ attention ÑĞ»Ğ¾Ğ¸
freeze_by_name(model, [".*attention.*", ".*attn.*"])
```

---

### Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ¸

#### unfreeze_model

```python
def unfreeze_model(model) -> int
```

Ğ Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹.

---

### Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°

#### get_trainable_params

```python
def get_trainable_params(model) -> int
```

ĞŸĞ¾Ğ´ÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹.

```python
from transformers import get_trainable_params

trainable = get_trainable_params(model)
print(f"Trainable: {trainable:,}")
```

---

#### get_frozen_percentage

```python
def get_frozen_percentage(model) -> float
```

ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².

```python
from transformers import get_frozen_percentage

pct = get_frozen_percentage(model)
print(f"Frozen: {pct:.1f}%")
```

---

#### get_num_layers

```python
def get_num_layers(model) -> int
```

ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ»Ğ¾Ñ‘Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.

```python
from transformers import get_num_layers

num = get_num_layers(model)
print(f"Model has {num} layers")
```

---

#### print_layer_status

```python
def print_layer_status(model, show_all: bool = False)
```

Ğ’Ñ‹Ğ²ĞµÑÑ‚Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° ÑĞ»Ğ¾Ñ‘Ğ².

```python
from transformers import print_layer_status

print_layer_status(model)
# ========================================
# LAYER STATUS
# ========================================
# Layer 0-15:  FROZEN  (1.2B params)
# Layer 16-31: ACTIVE  (1.2B params)
# ----------------------------------------
# Embeddings:  FROZEN
# LM Head:     ACTIVE
# ========================================
```

---

### GradualUnfreezer

```python
class GradualUnfreezer:
    def __init__(
        self,
        model,
        total_epochs: int,
        unfreeze_embeddings_at: int = None,
        freeze_embeddings: bool = True,
        verbose: bool = True
    )
```

ĞŸĞ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ° ÑĞ»Ğ¾Ñ‘Ğ² Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ transfer learning.

```python
from transformers import GradualUnfreezer

unfreezer = GradualUnfreezer(model, total_epochs=10)

for epoch in range(10):
    unfreezer.step(epoch)  # Ğ Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾
    train_one_epoch(model, ...)
```

---

### setup_lp_lora_style

```python
def setup_lp_lora_style(
    model,
    freeze_ratio: float = 0.5,
    freeze_embed: bool = True
) -> int
```

Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° LP-LoRA ÑÑ‚Ğ¸Ğ»Ñ.

```python
from transformers import setup_lp_lora_style

# Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ 50% ÑĞ»Ğ¾Ñ‘Ğ² + ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸
setup_lp_lora_style(model, freeze_ratio=0.5)
```

---

### get_memory_savings_estimate

```python
def get_memory_savings_estimate(model) -> Dict[str, float]
```

ĞÑ†ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ¸.

```python
from transformers import get_memory_savings_estimate

savings = get_memory_savings_estimate(model)
print(f"Gradient memory saved: {savings['gradient_saved_gb']:.2f} GB")
print(f"Optimizer memory saved: {savings['optimizer_saved_gb']:.2f} GB")
```

---

## ğŸ¯ Ğ¢Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸

### LP-LoRA ÑÑ‚Ğ¸Ğ»ÑŒ

```python
from transformers import setup_lp_lora_style

setup_lp_lora_style(model, freeze_ratio=0.5)
# Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 50% ÑĞ»Ğ¾Ñ‘Ğ² Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ñ‹
```

### ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾Ñ‘Ğ²

```python
from transformers import freeze_except_last_n

freeze_except_last_n(model, n=4)
# Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 4 ÑĞ»Ğ¾Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ
```

### ĞŸĞ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ°

```python
from transformers import GradualUnfreezer

unfreezer = GradualUnfreezer(model, total_epochs=10)
# Ğ­Ğ¿Ğ¾Ñ…Ğ° 1: Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ lm_head
# Ğ­Ğ¿Ğ¾Ñ…Ğ° 5: Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ° ÑĞ»Ğ¾Ñ‘Ğ²
# Ğ­Ğ¿Ğ¾Ñ…Ğ° 10: Ğ²ÑĞµ ÑĞ»Ğ¾Ğ¸
```

---

## âš ï¸ Ğ’Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ñ

1. **Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ:** Ğ’ÑĞµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‚ `requires_grad`, Ğ½Ğµ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ Ğ²ĞµÑĞ°
2. **ĞĞ±Ñ€Ğ°Ñ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ:** ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ²Ñ‹Ğ·Ğ²Ğ°Ñ‚ÑŒ `unfreeze_model()`
3. **Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ:** Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ»ÑĞ±Ğ¾Ğ¹ PyTorch Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ

---

## ğŸ†• ĞĞ¾Ğ²Ñ‹Ğµ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ v1.0.6

### get_layer_names

```python
def get_layer_names(model, include_params: bool = False) -> List[str]
```

ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¸Ğ¼Ñ‘Ğ½ Ğ²ÑĞµÑ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.

```python
from transformers.layer_utils import get_layer_names

# Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸
names = get_layer_names(model)
# ['embed', 'layer1', 'layer2', 'head']

# Ğ¡ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸
names_params = get_layer_names(model, include_params=True)
# ['embed.weight', 'layer1.weight', 'layer1.bias', ...]
```

---

### estimate_training_time

```python
def estimate_training_time(
    model,
    num_samples: int,
    batch_size: int,
    num_epochs: int,
    ms_per_step: float = 500.0
) -> Dict[str, Any]
```

ĞÑ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸.

```python
from transformers.layer_utils import estimate_training_time

estimate = estimate_training_time(
    model=model,
    num_samples=50000,
    batch_size=16,
    num_epochs=3
)

print(f"Total steps: {estimate['total_steps']}")
print(f"Estimated time: {estimate['formatted']}")
# Total steps: 9375
# Estimated time: 1h 18m 7s
```

---

### print_model_summary

```python
def print_model_summary(model, max_depth: int = 3) -> None
```

ĞšÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â€” ĞºĞ°Ğº `model.summary()` Ğ² Keras.

```python
from transformers.layer_utils import print_model_summary

print_model_summary(model)
```

**Ğ’Ñ‹Ğ²Ğ¾Ğ´:**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ“Š MODEL SUMMARY                                                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Model: GPT2LMHeadModel                                                â•‘
â•‘  Total Parameters: 124,439,808                                         â•‘
â•‘  Trainable: 124,439,808 (100.0%)                                       â•‘
â•‘  Frozen: 0 (0.0%)                                                      â•‘
â•‘  Memory: ~475.0 MB                                                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Layer                                â”‚   Parameters â”‚ Status          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  transformer.wte                      â”‚   38,597,376 â”‚ âœ“               â•‘
â•‘  transformer.wpe                      â”‚      786,432 â”‚ âœ“               â•‘
â•‘  transformer.h.0                      â”‚    7,087,872 â”‚ âœ“               â•‘
â•‘  ...                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

- **âœ“** = trainable (Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹)
- **âœ—** = frozen (Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹)

