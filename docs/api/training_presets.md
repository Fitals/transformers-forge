# Training Presets

**–ú–æ–¥—É–ª—å:** `transformers.training_presets`

–ì–æ—Ç–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ç–∏–ø–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –æ–±—É—á–µ–Ω–∏—è LLM.

---

## üìä –ó–∞—á–µ–º –Ω—É–∂–Ω—ã Presets?

| –ü—Ä–æ–±–ª–µ–º–∞ | –†–µ—à–µ–Ω–∏–µ |
|----------|---------|
| –î–æ–ª–≥–æ –ø–æ–¥–±–∏—Ä–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã | –ì–æ—Ç–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏ |
| –ó–∞–ø—É—Ç–∞—Ç—å—Å—è –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö LoRA | –ü—Ä–µ—Å–µ—Ç —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω |
| –ù–µ –∑–Ω–∞—é —Å —á–µ–≥–æ –Ω–∞—á–∞—Ç—å | –í—ã–±–µ—Ä–∏ preset –∏ –∑–∞–ø—É—Å–∫–∞–π |

---

## üîß –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```python
from transformers import get_preset

# –ü–æ–ª—É—á–∏—Ç—å preset –¥–ª—è QLoRA
preset = get_preset("qlora")

# –ü–æ–ª—É—á–∏—Ç—å –≥–æ—Ç–æ–≤—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
training_args = preset.get_training_args()
lora_config = preset.get_lora_config()
bnb_config = preset.get_bnb_config()
```

---

## üìñ –î–æ—Å—Ç—É–ø–Ω—ã–µ Presets

| Preset | –û–ø–∏—Å–∞–Ω–∏–µ | –ü–∞–º—è—Ç—å | –°–∫–æ—Ä–æ—Å—Ç—å |
|--------|----------|--------|----------|
| `sft` | Supervised Fine-Tuning | –í—ã—Å–æ–∫–∞—è | –ë—ã—Å—Ç—Ä–æ |
| `lora` | LoRA fine-tuning | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω–µ |
| `qlora` | QLoRA (4-bit) | –ù–∏–∑–∫–∞—è | –ú–µ–¥–ª–µ–Ω–Ω–µ–µ |
| `dpo` | Direct Preference Optimization | –í—ã—Å–æ–∫–∞—è | –°—Ä–µ–¥–Ω–µ |
| `memory_efficient` | –ú–∏–Ω–∏–º—É–º –ø–∞–º—è—Ç–∏ | –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è | –ú–µ–¥–ª–µ–Ω–Ω–æ |

---

## üìñ API Reference

### get_preset

```python
def get_preset(
    name: str,
    **overrides
) -> BasePreset
```

–ü–æ–ª—É—á–∏—Ç—å preset –ø–æ –∏–º–µ–Ω–∏ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

```python
from transformers import get_preset

# –ë–∞–∑–æ–≤—ã–π preset
preset = get_preset("qlora")

# –° –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏
preset = get_preset("qlora", lora_r=64, learning_rate=1e-4)
```

---

### –ú–µ—Ç–æ–¥—ã Preset

–ö–∞–∂–¥—ã–π preset –∏–º–µ–µ—Ç –º–µ—Ç–æ–¥—ã:

| –ú–µ—Ç–æ–¥ | –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç | –û–ø–∏—Å–∞–Ω–∏–µ |
|-------|------------|----------|
| `get_training_args()` | TrainingArguments | –ê—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è Trainer |
| `get_args_dict()` | Dict | –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–±–µ–∑ accelerate) *(v1.1.0)* |
| `get_lora_config()` | LoraConfig | –ö–æ–Ω—Ñ–∏–≥ LoRA (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ) |
| `get_bnb_config()` | BitsAndBytesConfig | –ö–æ–Ω—Ñ–∏–≥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ) |
| `print_info()` | None | –í—ã–≤–µ—Å—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ preset |

---

## üéØ –ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ Presets

### SFT Preset

```python
preset = get_preset("sft")
```

**–î–ª—è:** –ü–æ–ª–Ω–æ–µ fine-tuning –±–µ–∑ LoRA.

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é:**
```python
{
    "learning_rate": 2e-5,
    "num_train_epochs": 3,
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "warmup_ratio": 0.1,
    "weight_decay": 0.01,
    "fp16": True,  # –ê–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è
    "gradient_checkpointing": True,
}
```

---

### LoRA Preset

```python
preset = get_preset("lora")
```

**–î–ª—è:** Fine-tuning —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏.

**LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**
```python
{
    "r": 16,
    "lora_alpha": 32,
    "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
    "lora_dropout": 0.05,
    "bias": "none",
}
```

---

### QLoRA Preset

```python
preset = get_preset("qlora")
```

**–î–ª—è:** 4-bit fine-tuning —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ–º –ø–∞–º—è—Ç–∏.

**BnB Config:**
```python
{
    "load_in_4bit": True,
    "bnb_4bit_compute_dtype": torch.bfloat16,
    "bnb_4bit_use_double_quant": True,
    "bnb_4bit_quant_type": "nf4",
}
```

---

### DPO Preset

```python
preset = get_preset("dpo")
```

**–î–ª—è:** Direct Preference Optimization (–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏).

**DPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**
```python
{
    "beta": 0.1,
    "learning_rate": 5e-7,
    "max_length": 1024,
    "max_prompt_length": 512,
}
```

---

### Memory Efficient Preset

```python
preset = get_preset("memory_efficient")
```

**–î–ª—è:** –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏.

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
- Gradient checkpointing
- –ú–∞–ª–µ–Ω—å–∫–∏–π batch size
- –í—ã—Å–æ–∫–∏–π gradient accumulation

---

## üí° –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer
from transformers import get_preset
from peft import get_peft_model

# 1. –ü–æ–ª—É—á–∞–µ–º preset
preset = get_preset("qlora", lora_r=32)

# 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    quantization_config=preset.get_bnb_config()
)

# 3. –î–æ–±–∞–≤–ª—è–µ–º LoRA
model = get_peft_model(model, preset.get_lora_config())

# 4. –°–æ–∑–¥–∞—ë–º Trainer
trainer = Trainer(
    model=model,
    args=preset.get_training_args(),
    train_dataset=dataset,
)

# 5. –û–±—É—á–∞–µ–º
trainer.train()
```

---

## üõ† –ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è

### –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

```python
preset = get_preset(
    "qlora",
    learning_rate=1e-4,
    lora_r=64,
    lora_alpha=128,
    num_train_epochs=5
)
```

### –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–µ–≥–æ preset

```python
from transformers.training_presets import BasePreset

class MyPreset(BasePreset):
    name = "my_preset"
    description = "–ú–æ–π –∫–∞—Å—Ç–æ–º–Ω—ã–π preset"
    
    def get_training_args(self):
        return TrainingArguments(
            output_dir="./output",
            learning_rate=1e-5,
            # ... –≤–∞—à–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        )
```

---

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

1. **–ê–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** Presets –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç GPU/CPU –∏ bf16/fp16
2. **PEFT –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å:** LoRA/QLoRA presets —Ç—Ä–µ–±—É—é—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É `peft`
3. **BnB –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å:** QLoRA —Ç—Ä–µ–±—É–µ—Ç `bitsandbytes`
