# Learning Rate Finder

**–ú–æ–¥—É–ª—å:** `transformers.lr_finder`

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ learning rate –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–æ–¥–∞ Leslie Smith (2015).

---

## üìä –ó–∞—á–µ–º –Ω—É–∂–µ–Ω LR Finder?

| –ü—Ä–æ–±–ª–µ–º–∞ | –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç | –†–µ—à–µ–Ω–∏–µ |
|----------|----------------|---------|
| LR —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π | Loss –≤–∑—Ä—ã–≤–∞–µ—Ç—Å—è, –º–æ–¥–µ–ª—å –ª–æ–º–∞–µ—Ç—Å—è | LR Finder –ø–æ–∫–∞–∂–µ—Ç —Ç–æ—á–∫—É –≤–∑—Ä—ã–≤–∞ |
| LR —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π | –ú–æ–¥–µ–ª—å –Ω–µ —É—á–∏—Ç—Å—è, –≤—Ä–µ–º—è –ø–æ—Ç—Ä–∞—á–µ–Ω–æ | LR Finder –Ω–∞–π–¥—ë—Ç –∑–æ–Ω—É –æ–±—É—á–µ–Ω–∏—è |
| –£–≥–∞–¥—ã–≤–∞–Ω–∏–µ LR | –ú–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ | –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –∑–∞ 2 –º–∏–Ω—É—Ç—ã |

---

## üîß –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```python
from transformers.lr_finder import find_optimal_lr

# –ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π LR –∑–∞ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
optimal_lr = find_optimal_lr(model, train_dataloader)
print(f"–ò—Å–ø–æ–ª—å–∑—É–π LR: {optimal_lr}")
```

---

## üìñ API Reference

### LRFinder

```python
class LRFinder:
    def __init__(
        self,
        model: torch.nn.Module,
        train_dataloader: DataLoader,
        optimizer: Optional[Optimizer] = None,
        criterion: Optional[callable] = None,
        device: str = "auto"
    )
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|--------------|----------|
| `model` | nn.Module | ‚Äî | PyTorch –º–æ–¥–µ–ª—å |
| `train_dataloader` | DataLoader | ‚Äî | –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è |
| `optimizer` | Optimizer | AdamW | –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) |
| `criterion` | callable | None | –§—É–Ω–∫—Ü–∏—è loss (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) |
| `device` | str | "auto" | –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: "auto", "cuda", "cpu" |

---

### –ú–µ—Ç–æ–¥—ã

#### find()

```python
def find(
    self,
    min_lr: float = 1e-8,
    max_lr: float = 1e-1,
    num_steps: int = 100,
    smooth_factor: float = 0.98,
    divergence_threshold: float = 4.0,
    suggestion_method: str = "steepest_gradient"
) -> LRFinderResult
```

–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ LR.

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|--------------|----------|
| `min_lr` | float | 1e-8 | –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π LR –¥–ª—è —Ç–µ—Å—Ç–∞ |
| `max_lr` | float | 1e-1 | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π LR –¥–ª—è —Ç–µ—Å—Ç–∞ |
| `num_steps` | int | 100 | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ |
| `smooth_factor` | float | 0.98 | –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ loss (0-1) |
| `divergence_threshold` | float | 4.0 | –ü–æ—Ä–æ–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–∏ –≤–∑—Ä—ã–≤–µ |
| `suggestion_method` | str | "steepest_gradient" | –ú–µ—Ç–æ–¥ –≤—ã–±–æ—Ä–∞ LR |

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `LRFinderResult`

---

#### plot()

```python
def plot(
    self,
    output_path: Optional[str] = None,
    log_scale: bool = True,
    show_suggestion: bool = True
) -> Optional[str]
```

–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫ loss vs learning rate.

```python
finder.plot("lr_curve.png")
```

---

#### reset()

```python
def reset(self)
```

–í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏.

---

### LRFinderResult

```python
@dataclass
class LRFinderResult:
    optimal_lr: float       # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π LR
    min_lr: float           # –ú–∏–Ω —Ç–µ—Å—Ç–∏—Ä—É–µ–º—ã–π LR
    max_lr: float           # –ú–∞–∫—Å —Ç–µ—Å—Ç–∏—Ä—É–µ–º—ã–π LR
    num_steps: int          # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
    lrs: List[float]        # –í—Å–µ LR
    losses: List[float]     # Loss –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ
    smoothed_losses: List[float]  # –°–≥–ª–∞–∂–µ–Ω–Ω—ã–µ
    best_lr_idx: int        # –ò–Ω–¥–µ–∫—Å –ª—É—á—à–µ–≥–æ LR
    suggestion_method: str  # –ú–µ—Ç–æ–¥ –≤—ã–±–æ—Ä–∞
```

---

## üí° –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.lr_finder import LRFinder
from torch.utils.data import DataLoader
from datasets import load_dataset

# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# 2. –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ
dataset = load_dataset("text", data_files="train.txt", split="train")

def tokenize(examples):
    return tokenizer(
        examples["text"], 
        truncation=True, 
        padding="max_length",
        max_length=128,
        return_tensors="pt"
    )

tokenized = dataset.map(tokenize, batched=True, remove_columns=["text"])
tokenized.set_format("torch", columns=["input_ids", "attention_mask"])

dataloader = DataLoader(tokenized, batch_size=4, shuffle=True)

# 3. –ó–∞–ø—É—Å–∫–∞–µ–º LR Finder
finder = LRFinder(model, dataloader)
result = finder.find(num_steps=100)

print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π LR: {result.optimal_lr:.2e}")

# 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
finder.plot("lr_finder.png")

# 5. –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π LR
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./output",
    learning_rate=result.optimal_lr,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π LR
    num_train_epochs=3,
)
```

---

## üî¨ –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç

1. **–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç LR**: –æ—Ç `min_lr` –¥–æ `max_lr`
2. **–ó–∞–ø–∏—Å—å loss**: –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ
3. **–°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ**: exponential moving average
4. **–ê–Ω–∞–ª–∏–∑**: –ø–æ–∏—Å–∫ —Ç–æ—á–∫–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–Ω–∏–∂–µ–Ω–∏—è loss
5. **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: LR —á—É—Ç—å –Ω–∏–∂–µ —ç—Ç–æ–π —Ç–æ—á–∫–∏

```
Loss
  ‚îÇ
  ‚îÇ\
  ‚îÇ \          ‚Üê –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π LR –∑–¥–µ—Å—å
  ‚îÇ  \        /
  ‚îÇ   \______/
  ‚îÇ           \
  ‚îÇ            \  ‚Üê Divergence
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ LR
    1e-7      1e-4     1e-1
```

---

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

1. **–í–µ—Å–∞ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è** ‚Äî –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–∞ –º–æ–¥–µ–ª—å –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏
2. **–ù—É–∂–µ–Ω GPU** ‚Äî –Ω–∞ CPU —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–æ
3. **Batch size –≤–∞–∂–µ–Ω** ‚Äî –ø—Ä–∏ OOM —É–º–µ–Ω—å—à–∏—Ç–µ batch size
4. **100 —à–∞–≥–æ–≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ** ‚Äî –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω–æ

---

## üìö –ù–∞—É—á–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ

- **Leslie Smith (2015)** ‚Äî "Cyclical Learning Rates for Training Neural Networks"
- **–°—Å—ã–ª–∫–∞:** https://arxiv.org/abs/1506.01186

–ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤:
- FastAI
- PyTorch Lightning
- Keras
- –ò —Ç–µ–ø–µ—Ä—å –≤ Transformers Forge!

---

## üÜï –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ v1.1.1

- –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π LR Finder
- –≠–∫—Å–ø–æ—Ä—Ç —á–µ—Ä–µ–∑ `from transformers import LRFinder`
- –ì—Ä–∞—Ñ–∏–∫ loss vs LR
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
