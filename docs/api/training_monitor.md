# Training Monitor

**–ú–æ–¥—É–ª—å:** `transformers.training_monitor`

–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –æ—Ç–ª–∞–¥–∫–∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.

---

## üìä –ó–∞—á–µ–º –Ω—É–∂–µ–Ω Training Monitor?

| –ü—Ä–æ–±–ª–µ–º–∞ | –†–µ—à–µ–Ω–∏–µ |
|----------|---------|
| –ù–µ –ø–æ–Ω–∏–º–∞—é —Å–∫–æ–ª—å–∫–æ –ø–∞–º—è—Ç–∏ –Ω—É–∂–Ω–æ | `estimate_model_memory()` |
| –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–∑—Ä—ã–≤–∞—é—Ç—Å—è/–∏—Å—á–µ–∑–∞—é—Ç | `check_gradient_health()` |
| –•–æ—á—É –≤–∏–¥–µ—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å | `MonitorCallback` |

---

## üîß –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```python
from transformers import Trainer
from transformers import MonitorCallback, print_model_info

# –í—ã–≤–µ—Å—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
print_model_info(model)

# –î–æ–±–∞–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ Trainer
trainer = Trainer(
    model=model,
    args=args,
    callbacks=[MonitorCallback(check_gradients=True)]
)
```

---

## üìñ API Reference

### –§—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ –º–æ–¥–µ–ª–∏

#### count_parameters

```python
def count_parameters(
    model,
    trainable_only: bool = False
) -> int
```

–ü–æ–¥—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.

```python
from transformers import count_parameters

total = count_parameters(model)
trainable = count_parameters(model, trainable_only=True)
print(f"Total: {total:,}, Trainable: {trainable:,}")
```

---

#### format_param_count

```python
def format_param_count(count: int) -> str
```

–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (1.5B, 7M, 3K).

```python
from transformers import format_param_count

print(format_param_count(1_500_000_000))  # "1.50B"
print(format_param_count(7_000_000))       # "7.00M"
```

---

#### get_parameter_breakdown

```python
def get_parameter_breakdown(model) -> List[Dict]
```

–î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Å–ª–æ—è–º.

```python
from transformers import get_parameter_breakdown

breakdown = get_parameter_breakdown(model)
for layer in breakdown[:5]:
    print(f"{layer['name']}: {layer['params']:,} ({layer['dtype']})")
```

---

#### estimate_model_memory

```python
def estimate_model_memory(
    model,
    batch_size: int = 1,
    sequence_length: int = 512,
    precision: str = "fp16"
) -> Dict[str, float]
```

–û—Ü–µ–Ω–∫–∞ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏.

```python
from transformers import estimate_model_memory

memory = estimate_model_memory(model, batch_size=4, sequence_length=2048)
print(f"Parameters: {memory['parameters_gb']:.2f} GB")
print(f"Gradients: {memory['gradients_gb']:.2f} GB")
print(f"Optimizer: {memory['optimizer_gb']:.2f} GB")
print(f"Total: {memory['total_estimated_gb']:.2f} GB")
```

---

#### print_model_info

```python
def print_model_info(
    model,
    show_breakdown: bool = False
)
```

–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏.

```python
from transformers import print_model_info

print_model_info(model)
# ================================================
# MODEL INFO
# ================================================
# Total parameters:     7,000,000,000 (7.00B)
# Trainable parameters: 1,000,000 (1.00M)
# Frozen parameters:    6,999,000,000
# Trainable ratio:      0.01%
# ================================================
# Memory Estimation (FP16):
# Parameters:           13.04 GB
# Gradients:            0.00 GB (frozen excluded)
# Optimizer (AdamW):    26.08 GB
# ================================================
```

---

### –§—É–Ω–∫—Ü–∏–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è

#### check_gradient_health

```python
def check_gradient_health(
    model,
    warn_threshold: float = 1.0,
    error_threshold: float = 10.0
) -> Dict
```

–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

```python
from transformers import check_gradient_health

# –ü–æ—Å–ª–µ backward() –Ω–æ –¥–æ optimizer.step()
health = check_gradient_health(model)

if not health["healthy"]:
    print("‚ö†Ô∏è Gradient issues detected!")
    for issue in health["issues"]:
        print(f"  - {issue}")
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:**
- `healthy`: bool ‚Äî –≤—Å—ë –ª–∏ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
- `issues`: list ‚Äî —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º
- `stats`: dict ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (min, max, mean, std)

---

### GradientStats

```python
@dataclass
class GradientStats:
    min_grad: float
    max_grad: float
    mean_grad: float
    std_grad: float
    num_zero: int
    num_nan: int
    num_inf: int
```

–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

---

### TrainingMetrics

```python
@dataclass  
class TrainingMetrics:
    step: int
    loss: float
    learning_rate: float
    gradient_norm: float
    gpu_memory_used: float
    gpu_memory_total: float
```

–ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è.

---

### TrainingMonitor

```python
class TrainingMonitor:
    def __init__(self, model, check_every: int = 100)
```

–ü–æ–ª–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è.

```python
from transformers import TrainingMonitor

monitor = TrainingMonitor(model)

# –ü–æ–ª—É—á–∏—Ç—å summary
summary = monitor.get_model_summary()
print(f"Total params: {summary['total_parameters']}")
print(f"Trainable: {summary['trainable_parameters']}")
```

---

### MonitorCallback

```python
class MonitorCallback(TrainerCallback):
    def __init__(
        self,
        print_model_summary: bool = True,
        log_gpu_memory: bool = True,
        check_gradients: bool = False,
        gradient_check_steps: int = 100
    )
```

Callback –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å `Trainer`.

```python
from transformers import Trainer
from transformers import MonitorCallback

trainer = Trainer(
    model=model,
    args=args,
    callbacks=[
        MonitorCallback(
            print_model_summary=True,
            log_gpu_memory=True,
            check_gradients=True
        )
    ]
)
```

**–§—É–Ω–∫—Ü–∏–∏ callback:**
- –ü—Ä–∏ —Å—Ç–∞—Ä—Ç–µ ‚Äî –≤—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
- –ö–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
- –õ–æ–≥–∏—Ä—É–µ—Ç GPU memory –≤ wandb/tensorboard

---

### GPU –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

#### get_gpu_memory_info

```python
def get_gpu_memory_info() -> Dict[str, float]
```

–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏ GPU.

```python
from transformers.training_monitor import get_gpu_memory_info

gpu_info = get_gpu_memory_info()
print(f"Used: {gpu_info['used_gb']:.2f} GB")
print(f"Free: {gpu_info['free_gb']:.2f} GB")
print(f"Total: {gpu_info['total_gb']:.2f} GB")
```

---

## üí° –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä

```python
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments
from transformers import (
    MonitorCallback,
    print_model_info,
    estimate_model_memory
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = AutoModelForCausalLM.from_pretrained("model_name")

# –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
print_model_info(model, show_breakdown=True)

memory = estimate_model_memory(model, batch_size=4, sequence_length=2048)
print(f"\nEstimated memory: {memory['total_estimated_gb']:.2f} GB")

# –û–±—É—á–µ–Ω–∏–µ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
trainer = Trainer(
    model=model,
    args=TrainingArguments(output_dir="./output"),
    train_dataset=dataset,
    callbacks=[
        MonitorCallback(
            check_gradients=True,
            gradient_check_steps=50
        )
    ]
)

trainer.train()
```

---

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

1. **GPU:** –§—É–Ω–∫—Ü–∏–∏ GPU —Ä–∞–±–æ—Ç–∞—é—Ç —Ç–æ–ª—å–∫–æ —Å CUDA
2. **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** `check_gradients` –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–µ–±–æ–ª—å—à–æ–π overhead
3. **–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å:** –†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±–æ–π PyTorch –º–æ–¥–µ–ª—å—é
