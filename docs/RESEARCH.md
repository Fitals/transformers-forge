# üìö –¢–µ–æ—Ä–∏—è –∏ –ü—Ä–∞–∫—Ç–∏–∫–∞ ‚Äî Transformers Forge

–ù–∞—É—á–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ Transformers Forge.

---

## üî¨ EMA (Exponential Moving Average)

### üìñ –¢–µ–æ—Ä–∏—è

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:**

EMA –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ä–∞–±–æ—Ç–µ **Polyak & Juditsky (1992)** "Acceleration of Stochastic Approximation by Averaging".

**–§–æ—Ä–º—É–ª–∞:**
```
Œ∏_ema(t) = Œ≤ √ó Œ∏_ema(t-1) + (1-Œ≤) √ó Œ∏(t)
```

**–ö–ª—é—á–µ–≤–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**
- –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π SGD —Å—Ö–æ–¥–∏—Ç—Å—è –∫–∞–∫ O(1/‚àöt)
- SGD —Å Polyak averaging —Å—Ö–æ–¥–∏—Ç—Å—è –∫–∞–∫ **O(1/t)** ‚Äî –±—ã—Å—Ç—Ä–µ–µ!

**–ü–æ—á–µ–º—É —Ä–∞–±–æ—Ç–∞–µ—Ç:**
1. SGD "–ø—Ä—ã–≥–∞–µ—Ç" –≤–æ–∫—Ä—É–≥ –æ–ø—Ç–∏–º—É–º–∞ –∏–∑-–∑–∞ —à—É–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
2. EMA —É—Å—Ä–µ–¥–Ω—è–µ—Ç —ç—Ç–∏ –∫–æ–ª–µ–±–∞–Ω–∏—è
3. –†–µ–∑—É–ª—å—Ç–∞—Ç: –º–µ–Ω—å—à–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è ‚Üí –ª—É—á—à–∞—è generalization

**–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–∏–≥—Ä—ã—à:**
| –ú–µ—Ç—Ä–∏–∫–∞ | –ë–µ–∑ EMA | –° EMA | –ò—Å—Ç–æ—á–Ω–∏–∫ |
|---------|---------|-------|----------|
| Variance | O(œÉ¬≤/t) | O(œÉ¬≤/t¬≤) | Polyak 1992 |
| Convergence | O(1/‚àöt) | O(1/t) | Ruppert 1988 |

**–ö–ª—é—á–µ–≤—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:**
- Polyak, B. T., & Juditsky, A. B. (1992). "Acceleration of stochastic approximation by averaging"
- Ruppert, D. (1988). "Efficient estimations from a slowly convergent Robbins-Monro process"
- Mandt, S., et al. (2017). "Stochastic Gradient Descent as Approximate Bayesian Inference"

**SOTA –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ EMA:**
- ‚úÖ Stable Diffusion (Stability AI)
- ‚úÖ DALL-E 2 (OpenAI)
- ‚úÖ Imagen (Google Brain)
- ‚úÖ EDM (Karras et al., 2022)

---

### üß™ –ü—Ä–∞–∫—Ç–∏–∫–∞

**–û–∂–∏–¥–∞–µ–º–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ:** +1-3% –Ω–∞ eval –º–µ—Ç—Ä–∏–∫–∞—Ö

| –°—Ç–∞—Ç—É—Å | –û–ø–∏—Å–∞–Ω–∏–µ |
|--------|----------|
| üî¥ **–ü—Ä–µ–¥—Å—Ç–æ–∏—Ç** | –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ benchmarks –Ω–∞ LLM fine-tuning |

**–ü–ª–∞–Ω–∏—Ä—É–µ–º—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã:**
- [ ] GPT-2 fine-tuning: loss comparison
- [ ] Mistral-7B QLoRA: perplexity comparison
- [ ] Ablation study: decay values (0.99, 0.999, 0.9999)

---

## üßä Layer Freezing (LP-LoRA —Å—Ç–∏–ª—å)

### üìñ –¢–µ–æ—Ä–∏—è

**–ö–æ–Ω—Ü–µ–ø—Ü–∏—è:**

–ù–∏–∂–Ω–∏–µ —Å–ª–æ–∏ transformer —Å–æ–¥–µ—Ä–∂–∞—Ç –±–æ–ª–µ–µ –æ–±—â–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è (syntax, basic semantics), –∞ –≤–µ—Ä—Ö–Ω–∏–µ ‚Äî task-specific –∑–Ω–∞–Ω–∏—è.

**Te–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:**
- **Clark et al. (2019)** "What Does BERT Look At?" ‚Äî –∞–Ω–∞–ª–∏–∑ attention patterns
- **Kovaleva et al. (2019)** "Revealing the Dark Secrets of BERT"
- **Guo et al. (2023)** "LongLoRA" ‚Äî —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å freezing

**–ü–æ—á–µ–º—É —Ä–∞–±–æ—Ç–∞–µ—Ç:**
1. –ù–∏–∂–Ω–∏–µ —Å–ª–æ–∏ —É–∂–µ —Ö–æ—Ä–æ—à–æ –æ–±—É—á–µ–Ω—ã –Ω–∞ pretrain
2. –ó–∞–º–æ—Ä–æ–∑–∫–∞ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç "catastrophic forgetting"
3. –ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ = –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ –Ω–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã

**LP-LoRA (Layer-wise Partial LoRA):**
- Freeze 50% –Ω–∏–∂–Ω–∏—Ö —Å–ª–æ—ë–≤
- LoRA —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–µ—Ä—Ö–Ω–∏—Ö —Å–ª–æ—è—Ö
- –†–µ–∑—É–ª—å—Ç–∞—Ç: —ç–∫–æ–Ω–æ–º–∏—è 50% –ø–∞–º—è—Ç–∏ –Ω–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã

**–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏:**
| % –∑–∞–º–æ—Ä–æ–∑–∫–∏ | –≠–∫–æ–Ω–æ–º–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ | –≠–∫–æ–Ω–æ–º–∏—è optimizer |
|-------------|---------------------|---------------------|
| 25% | 25% | 25% |
| 50% | 50% | 50% |
| 75% | 75% | 75% |

---

### üß™ –ü—Ä–∞–∫—Ç–∏–∫–∞

| –°—Ç–∞—Ç—É—Å | –û–ø–∏—Å–∞–Ω–∏–µ |
|--------|----------|
| üî¥ **–ü—Ä–µ–¥—Å—Ç–æ–∏—Ç** | Benchmarks memory usage –∏ quality |

**–ü–ª–∞–Ω–∏—Ä—É–µ–º—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã:**
- [ ] Memory profiling: 0% vs 50% vs 75% frozen
- [ ] Quality comparison: frozen vs full fine-tuning
- [ ] Optimal freeze ratio –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á

---

## ‚öôÔ∏è Training Presets

### üìñ –¢–µ–æ—Ä–∏—è

**–ö–æ–Ω—Ü–µ–ø—Ü–∏—è:**

–ì–æ—Ç–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ best practices –∏–∑:
- Hugging Face PEFT documentation
- QLoRA paper (Dettmers et al., 2023)
- TRL library defaults

**SFT (Supervised Fine-Tuning):**
- Learning rate: 2e-5 (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è BERT-style –º–æ–¥–µ–ª–µ–π)
- Warmup: 10% (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç destabilization –≤ –Ω–∞—á–∞–ª–µ)
- Cosine schedule (–ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ lr)

**LoRA parameters:**
- r=16: –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–æ/—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (Hu et al., 2021)
- lora_alpha=32: —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π alpha=2*r
- target_modules: q,k,v,o projections (–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç)

**QLoRA:**
- NF4 quantization: –ª—É—á—à–µ —á–µ–º INT4 –¥–ª—è weights (Dettmers 2023)
- Double quantization: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
- BF16 compute: –ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å —á–µ–º FP16

---

### üß™ –ü—Ä–∞–∫—Ç–∏–∫–∞

| –°—Ç–∞—Ç—É—Å | –û–ø–∏—Å–∞–Ω–∏–µ |
|--------|----------|
| üî¥ **–ü—Ä–µ–¥—Å—Ç–æ–∏—Ç** | –°—Ä–∞–≤–Ω–µ–Ω–∏–µ presets –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö |

**–ü–ª–∞–Ω–∏—Ä—É–µ–º—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã:**
- [ ] SFT vs LoRA vs QLoRA: quality/speed tradeoff
- [ ] Preset defaults vs custom tuning
- [ ] Memory usage comparison

---

## üìä Training Monitor

### üìñ –¢–µ–æ—Ä–∏—è

**–ö–æ–Ω—Ü–µ–ø—Ü–∏—è:**

–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º:

**Gradient Health:**
- **Vanishing gradients**: norm < 1e-7 (–º–æ–¥–µ–ª—å –Ω–µ —É—á–∏—Ç—Å—è)
- **Exploding gradients**: norm > 1e3 (–Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)
- **NaN/Inf**: –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞

**Memory Estimation:**
- Parameters: 4 bytes √ó num_params (FP32) –∏–ª–∏ 2 bytes (FP16)
- Gradients: —Ç–∞–∫–æ–π –∂–µ —Ä–∞–∑–º–µ—Ä –∫–∞–∫ parameters (–¥–ª—è trainable)
- Optimizer: 2x gradients –¥–ª—è Adam/AdamW (momentum + variance)

---

### üß™ –ü—Ä–∞–∫—Ç–∏–∫–∞

| –°—Ç–∞—Ç—É—Å | –û–ø–∏—Å–∞–Ω–∏–µ |
|--------|----------|
| ‚úÖ **–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ** | Unit —Ç–µ—Å—Ç—ã –Ω–∞ gradient detection |
| üî¥ **–ü—Ä–µ–¥—Å—Ç–æ–∏—Ç** | Real-world monitoring –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è |

---

## üìà –û–±—â–∏–π —Å—Ç–∞—Ç—É—Å

| –ú–æ–¥—É–ª—å | –¢–µ–æ—Ä–∏—è | –ü—Ä–∞–∫—Ç–∏–∫–∞ | –¢–µ—Å—Ç—ã |
|--------|--------|----------|-------|
| EMA | ‚úÖ –î–æ–∫–∞–∑–∞–Ω–æ | üî¥ –ü—Ä–µ–¥—Å—Ç–æ–∏—Ç | ‚úÖ –†–∞–±–æ—Ç–∞—é—Ç |
| Layer Freezing | ‚úÖ –û–±–æ—Å–Ω–æ–≤–∞–Ω–æ | üî¥ –ü—Ä–µ–¥—Å—Ç–æ–∏—Ç | ‚úÖ –†–∞–±–æ—Ç–∞—é—Ç |
| Training Presets | ‚úÖ Best practices | üî¥ –ü—Ä–µ–¥—Å—Ç–æ–∏—Ç | ‚úÖ –†–∞–±–æ—Ç–∞—é—Ç |
| Training Monitor | ‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ | üî¥ –ü—Ä–µ–¥—Å—Ç–æ–∏—Ç | ‚úÖ –†–∞–±–æ—Ç–∞—é—Ç |

---

## üìö –ë–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è

1. Polyak, B. T., & Juditsky, A. B. (1992). "Acceleration of stochastic approximation by averaging". SIAM Journal on Control and Optimization.

2. Hu, E. J., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models". arXiv:2106.09685.

3. Dettmers, T., et al. (2023). "QLoRA: Efficient Finetuning of Quantized LLMs". arXiv:2305.14314.

4. Karras, T., et al. (2022). "Elucidating the Design Space of Diffusion-Based Generative Models". NeurIPS.

5. Clark, K., et al. (2019). "What Does BERT Look At? An Analysis of BERT's Attention". BlackboxNLP.

---

**–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ:** –î–µ–∫–∞–±—Ä—å 2025
