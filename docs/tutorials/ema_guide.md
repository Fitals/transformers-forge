# üìà –ì–∞–π–¥ –ø–æ EMA

EMA (Exponential Moving Average) ‚Äî –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π.

---

## üéØ –ß—Ç–æ —Ç–∞–∫–æ–µ EMA?

EMA –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç **—Å–≥–ª–∞–∂–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏**:

```
EMA_–≤–µ—Å–∞ = 0.999 √ó EMA_–≤–µ—Å–∞ + 0.001 √ó —Ç–µ–∫—É—â–∏–µ_–≤–µ—Å–∞
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** EMA –≤–µ—Å–∞ –æ–±—ã—á–Ω–æ –¥–∞—é—Ç –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ eval, —á–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –æ–±—É—á–µ–Ω–∏—è.

---

## üìä –ü–æ—á–µ–º—É EMA —Ä–∞–±–æ—Ç–∞–µ—Ç?

| –ü—Ä–æ–±–ª–µ–º–∞ –æ–±—ã—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è | –ö–∞–∫ —Ä–µ—à–∞–µ—Ç EMA |
|---------------------------|----------------|
| –í–µ—Å–∞ "–ø—Ä—ã–≥–∞—é—Ç" –º–µ–∂–¥—É batch'–∞–º–∏ | EMA —É—Å—Ä–µ–¥–Ω—è–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è |
| –®—É–º –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö | –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ —É–±–∏—Ä–∞–µ—Ç —à—É–º |
| –ü–æ—Å–ª–µ–¥–Ω–∏–π batch –±—ã–ª –ø–ª–æ—Ö–æ–π | EMA –ø–æ–º–Ω–∏—Ç –≤—Å—é –∏—Å—Ç–æ—Ä–∏—é |

**–¢–∏–ø–∏—á–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ:** +1-3% –Ω–∞ eval –º–µ—Ç—Ä–∏–∫–∞—Ö

---

## üîß –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä

```python
from transformers import Trainer, TrainingArguments
from transformers.ema import EMACallback

# 1. –°–æ–∑–¥–∞—ë–º callback
ema_callback = EMACallback(decay=0.999)

# 2. –û–±—ã—á–Ω—ã–π Trainer
trainer = Trainer(
    model=model,
    args=TrainingArguments(
        output_dir="./output",
        num_train_epochs=3,
        per_device_train_batch_size=4,
    ),
    train_dataset=dataset,
    callbacks=[ema_callback]
)

# 3. –û–±—É—á–∞–µ–º
trainer.train()

# 4. –ü—Ä–∏–º–µ–Ω—è–µ–º EMA –≤–µ—Å–∞
ema_callback.apply_ema(model)

# 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å —Å EMA –≤–µ—Å–∞–º–∏
model.save_pretrained("./best_model")
```

---

### –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–∏–º–µ—Ä —Å EMAModel

```python
from transformers.ema import EMAModel
import torch

# –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ training loop
ema = EMAModel(model, decay=0.999)

for epoch in range(3):
    for batch in dataloader:
        # Forward
        outputs = model(**batch)
        loss = outputs.loss
        
        # Backward
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º EMA
        ema.update()
    
    # Eval —Å EMA –≤–µ—Å–∞–º–∏
    with ema.use_ema():
        eval_results = evaluate(model, eval_dataset)
        print(f"Epoch {epoch}: {eval_results}")

# –§–∏–Ω–∞–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ–º EMA
ema.apply_ema()
```

---

## üéõ –í—ã–±–æ—Ä decay

| –î–ª–∏–Ω–∞ –æ–±—É—á–µ–Ω–∏—è | Decay | –ü–æ—á–µ–º—É |
|----------------|-------|--------|
| ~1,000 —à–∞–≥–æ–≤ | 0.99 | –ë—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è |
| ~10,000 —à–∞–≥–æ–≤ | 0.999 | –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –≤—ã–±–æ—Ä |
| ~100,000 —à–∞–≥–æ–≤ | 0.9999 | –ú–µ–¥–ª–µ–Ω–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è |

**–ü—Ä–∞–≤–∏–ª–æ:** –ß–µ–º –¥–ª–∏–Ω–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ, —Ç–µ–º –±–ª–∏–∂–µ decay –∫ 1.0

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á—ë—Ç

```python
from transformers.ema import compute_optimal_decay

decay = compute_optimal_decay(total_steps=10000)
print(f"Recommended: {decay}")  # ~0.9993
```

---

## üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞

```python
import torch

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ EMA state
ema_state = ema_callback.get_ema_state()
torch.save({
    'model': model.state_dict(),
    'ema': ema_state
}, 'checkpoint.pt')

# –ó–∞–≥—Ä—É–∑–∫–∞
checkpoint = torch.load('checkpoint.pt')
model.load_state_dict(checkpoint['model'])
ema_callback.load_ema_state(checkpoint['ema'])
ema_callback.apply_ema(model)
```

---

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã

### –ü–∞–º—è—Ç—å

EMA —Ç—Ä–µ–±—É–µ—Ç **–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ø–∞–º—è—Ç—å** –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–ø–∏–∏ –≤–µ—Å–æ–≤.

```
–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: ~—Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤ –±–∞–π—Ç–∞—Ö
```

–î–ª—è 7B –º–æ–¥–µ–ª–∏ –≤ fp16: ~14 GB –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ.

### –ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å EMA

1. ‚úÖ **–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è** ‚Äî –¥–ª—è inference
2. ‚úÖ **–ù–∞ eval** ‚Äî –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
3. ‚ùå **–í–æ –≤—Ä–µ–º—è train forward** ‚Äî –Ω–µ –Ω—É–∂–Ω–æ

### –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ HuggingFace
- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç —Å LoRA/QLoRA
- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç —Å distributed training
- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç —Å gradient checkpointing

---

## üî¨ –ö–æ–≥–¥–∞ EMA –ø–æ–º–æ–≥–∞–µ—Ç –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ

1. **–ú–∞–ª—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã** ‚Äî EMA —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ
2. **–í—ã—Å–æ–∫–∏–π learning rate** ‚Äî EMA —Å–≥–ª–∞–∂–∏–≤–∞–µ—Ç –∫–æ–ª–µ–±–∞–Ω–∏—è
3. **–î–ª–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ** ‚Äî –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç —Å–∏–ª—å–Ω–µ–µ
4. **–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏** ‚Äî –æ—Å–æ–±–µ–Ω–Ω–æ –∑–∞–º–µ—Ç–Ω–æ –≤ text generation

---

## üìö –¢–µ–æ—Ä–∏—è

EMA –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ä–∞–±–æ—Ç–µ Polyak (1990) "Averaging in Stochastic Optimization".

–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏:
```
Œ∏_ema^(t) = Œ≤ √ó Œ∏_ema^(t-1) + (1-Œ≤) √ó Œ∏^(t)
```

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤:
- Stable Diffusion
- DALL-E 2
- Imagen
- –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ SOTA image –º–æ–¥–µ–ª–µ–π

---

## üéØ –†–µ–∑—é–º–µ

| –£—Å–∏–ª–∏–µ | –†–µ–∑—É–ª—å—Ç–∞—Ç |
|--------|-----------|
| 3 —Å—Ç—Ä–æ–∫–∏ –∫–æ–¥–∞ | +1-3% –∫–∞—á–µ—Å—Ç–≤–∞ |

```python
from transformers.ema import EMACallback
ema = EMACallback(decay=0.999)
# –î–æ–±–∞–≤–∏—Ç—å –≤ callbacks –∏ –≤—ã–∑–≤–∞—Ç—å apply_ema() –≤ –∫–æ–Ω—Ü–µ
```
