# üéì Fine-tuning —Å Transformers Forge

–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ fine-tuning LLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π Transformers Forge.

---

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞](#–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞)
2. [QLoRA fine-tuning](#qlora-fine-tuning)
3. [–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏](#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è-–ø–∞–º—è—Ç–∏)
4. [–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Å EMA](#—É–ª—É—á—à–µ–Ω–∏–µ-–∫–∞—á–µ—Å—Ç–≤–∞-—Å-ema)
5. [–ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä](#–ø–æ–ª–Ω—ã–π-–ø—Ä–∏–º–µ—Ä)

---

## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install -e .
pip install peft bitsandbytes accelerate datasets
```

### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö

```python
from datasets import load_dataset

dataset = load_dataset("your_dataset")
```

---

## QLoRA Fine-tuning

### –®–∞–≥ 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ preset

```python
from transformers import get_preset

# –ü–æ–ª—É—á–∞–µ–º –≥–æ—Ç–æ–≤—ã–π preset
preset = get_preset("qlora", lora_r=32, learning_rate=2e-4)

# –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
preset.print_info()
```

### –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    quantization_config=preset.get_bnb_config(),
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
```

### –®–∞–≥ 3: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ LoRA

```python
from peft import prepare_model_for_kbit_training, get_peft_model

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, preset.get_lora_config())
```

### –®–∞–≥ 4: –û–±—É—á–µ–Ω–∏–µ

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=preset.get_training_args(),
    train_dataset=dataset["train"],
    tokenizer=tokenizer,
)

trainer.train()
```

---

## –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏

### –ó–∞–º–æ—Ä–æ–∑–∫–∞ —Å–ª–æ—ë–≤ (LP-LoRA —Å—Ç–∏–ª—å)

```python
from transformers import setup_lp_lora_style, get_memory_savings_estimate

# –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å 50% —Å–ª–æ—ë–≤
setup_lp_lora_style(model, freeze_ratio=0.5)

# –û—Ü–µ–Ω–∏—Ç—å —ç–∫–æ–Ω–æ–º–∏—é
savings = get_memory_savings_estimate(model)
print(f"Gradient memory saved: {savings['gradient_saved_gb']:.2f} GB")
```

### –ê–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º

```python
from transformers import print_model_info, estimate_model_memory

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
print_model_info(model, show_breakdown=True)

# –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏
memory = estimate_model_memory(model, batch_size=4, sequence_length=2048)
print(f"Estimated total: {memory['total_estimated_gb']:.2f} GB")
```

---

## –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Å EMA

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ EMA

```python
from transformers.ema import EMACallback

ema_callback = EMACallback(decay=0.999)

trainer = Trainer(
    model=model,
    args=preset.get_training_args(),
    train_dataset=dataset["train"],
    callbacks=[ema_callback]  # –î–æ–±–∞–≤–ª—è–µ–º EMA
)

trainer.train()

# –ü—Ä–∏–º–µ–Ω—è–µ–º EMA –≤–µ—Å–∞
ema_callback.apply_ema(model)
```

---

## –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä

–ü–æ–ª–Ω—ã–π —Å–∫—Ä–∏–ø—Ç `train_with_forge.py`:

```python
"""
Transformers Forge: –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä fine-tuning
"""

from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    Trainer,
    get_preset,
    print_model_info,
    setup_lp_lora_style,
    MonitorCallback
)
from transformers.ema import EMACallback
from peft import prepare_model_for_kbit_training, get_peft_model

# =============================================================================
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# =============================================================================

MODEL_NAME = "mistralai/Mistral-7B-v0.1"
DATASET_NAME = "your_dataset"
OUTPUT_DIR = "./output"

# =============================================================================
# 1. –ü–æ–ª—É—á–∞–µ–º preset
# =============================================================================

preset = get_preset(
    "qlora",
    lora_r=32,
    lora_alpha=64,
    learning_rate=2e-4,
    num_train_epochs=3
)

print("üì¶ Preset configuration:")
preset.print_info()

# =============================================================================
# 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
# =============================================================================

print(f"\nüì• Loading model: {MODEL_NAME}")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=preset.get_bnb_config(),
    device_map="auto"
)

# =============================================================================
# 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏
# =============================================================================

print("\nüîß Preparing model...")

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, preset.get_lora_config())

# LP-LoRA —Å—Ç–∏–ª—å: –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å —á–∞—Å—Ç—å —Å–ª–æ—ë–≤
setup_lp_lora_style(model, freeze_ratio=0.5)

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
print_model_info(model)

# =============================================================================
# 4. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# =============================================================================

print(f"\nüìö Loading dataset: {DATASET_NAME}")

dataset = load_dataset(DATASET_NAME)

def tokenize(example):
    return tokenizer(
        example["text"],
        truncation=True,
        max_length=2048,
        padding="max_length"
    )

dataset = dataset.map(tokenize, batched=True)

# =============================================================================
# 5. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ callbacks
# =============================================================================

ema_callback = EMACallback(decay=0.999)
monitor_callback = MonitorCallback(check_gradients=True)

# =============================================================================
# 6. Trainer
# =============================================================================

training_args = preset.get_training_args()
training_args.output_dir = OUTPUT_DIR

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset.get("validation"),
    tokenizer=tokenizer,
    callbacks=[ema_callback, monitor_callback]
)

# =============================================================================
# 7. –û–±—É—á–µ–Ω–∏–µ
# =============================================================================

print("\nüöÄ Starting training...")
trainer.train()

# =============================================================================
# 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
# =============================================================================

print("\nüíæ Saving model with EMA weights...")

# –ü—Ä–∏–º–µ–Ω—è–µ–º EMA –≤–µ—Å–∞ (–ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ)
ema_callback.apply_ema(model)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
model.save_pretrained(f"{OUTPUT_DIR}/final_model")
tokenizer.save_pretrained(f"{OUTPUT_DIR}/final_model")

print(f"\n‚úÖ Done! Model saved to {OUTPUT_DIR}/final_model")
```

---

## üéØ –ß–µ–∫–ª–∏—Å—Ç fine-tuning

- [ ] –í—ã–±—Ä–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π preset (sft/lora/qlora/dpo)
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Ü–µ–Ω–∫—É –ø–∞–º—è—Ç–∏ `estimate_model_memory()`
- [ ] –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å —á–∞—Å—Ç—å —Å–ª–æ—ë–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
- [ ] –î–æ–±–∞–≤–∏—Ç—å EMA –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
- [ ] –î–æ–±–∞–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
- [ ] –ü—Ä–∏–º–µ–Ω–∏—Ç—å EMA –≤–µ—Å–∞ –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º

---

## üí° –°–æ–≤–µ—Ç—ã

1. **–ù–∞—á–Ω–∏—Ç–µ —Å QLoRA** ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞—Å—Ö–æ–¥ –ø–∞–º—è—Ç–∏
2. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ EMA** ‚Äî –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ +1-3% –∫–∞—á–µ—Å—Ç–≤–∞
3. **–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–π—Ç–µ —Å–ª–æ–∏** ‚Äî LP-LoRA –¥–∞—ë—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
4. **–ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã** ‚Äî –ª–æ–≤–∏—Ç–µ –ø—Ä–æ–±–ª–µ–º—ã —Ä–∞–Ω–æ
