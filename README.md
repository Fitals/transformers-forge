# üî® Transformers Forge

**–ù–µ–∑–∞–≤–∏—Å–∏–º—ã–π —Ñ–æ—Ä–∫ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Transformers —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è**

[![Version](https://img.shields.io/badge/version-1.0.7-blue.svg)](CHANGELOG.md)
[![Tests](https://github.com/Fitals/transformers-forge/actions/workflows/forge-unit-tests.yml/badge.svg)](https://github.com/Fitals/transformers-forge/actions/workflows/forge-unit-tests.yml)
[![License](https://img.shields.io/badge/license-Apache%202.0-green.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.10+-yellow.svg)](https://python.org)

---

## üìã –û –ø—Ä–æ–µ–∫—Ç–µ

**Transformers Forge** ‚Äî "–ö—É–∑–Ω–∏—Ü–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤". –ù–µ–∑–∞–≤–∏—Å–∏–º—ã–π —Ñ–æ—Ä–∫ —Å:
- üî• **EMA** ‚Äî —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ +1-3% (–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ)
- ‚ö° **Layer Utils** ‚Äî –∑–∞–º–æ—Ä–æ–∑–∫–∞ —Å–ª–æ—ë–≤, LP-LoRA —Å—Ç–∏–ª—å
- üì¶ **Training Presets** ‚Äî –≥–æ—Ç–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏ –¥–ª—è SFT/LoRA/QLoRA/DPO
- üìä **Training Monitor** ‚Äî –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU, –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
- üêõ **7 Bug Fixes** ‚Äî –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```bash
# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
git clone https://github.com/transformers-forge/transformers-forge.git
cd transformers-forge

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
pip install -e .
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏

```python
import transformers
print(transformers.__version__)
# Output: 1.0.0
```

---

## üî• –ì–ª–∞–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### 1. EMA ‚Äî –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ +1-3%

```python
from transformers import Trainer
from transformers.ema import EMACallback

trainer = Trainer(
    model=model,
    args=args,
    callbacks=[EMACallback(decay=0.999)]  # +1-3% quality!
)
trainer.train()

# –ü—Ä–∏–º–µ–Ω–∏—Ç—å EMA –≤–µ—Å–∞
ema_callback.apply_ema(model)
```

### 2. Layer Utils ‚Äî –ó–∞–º–æ—Ä–æ–∑–∫–∞ —Å–ª–æ—ë–≤

```python
from transformers import freeze_first_n_layers, get_frozen_percentage

# LP-LoRA —Å—Ç–∏–ª—å: –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å 50% —Å–ª–æ—ë–≤
freeze_first_n_layers(model, n=16)
print(f"Frozen: {get_frozen_percentage(model):.1f}%")  # ~50%
```

### 3. Training Presets ‚Äî –ì–æ—Ç–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏

```python
from transformers import get_preset

# –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç —Å LoRA
preset = get_preset("qlora", lora_r=32)
training_args = preset.get_training_args()
lora_config = preset.get_lora_config()
bnb_config = preset.get_bnb_config()
```

### 4. Training Monitor ‚Äî –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

```python
from transformers import TrainingMonitor, MonitorCallback

trainer = Trainer(
    model=model,
    callbacks=[MonitorCallback(check_gradients=True)]
)
```

---

## üêõ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –±–∞–≥–∏

| # | Issue | –ü—Ä–æ–±–ª–µ–º–∞ |
|---|-------|----------|
| 1 | #42925 | TvpConfig `type_vocab_size` |
| 2 | #42910 | Qwen2VL `size` parameter |
| 3 | #42679 | ConditionalDetr segmentation |
| 4 | #42722 | OneFormer device sync |
| 5 | #42890 | SAM HQ reproducibility |
| 6 | #42759 | Siglip `hidden_states` |
| 7 | #42762 | **GenerationConfig override** ‚≠ê |

üìñ **–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏:** [CHANGELOG.md](CHANGELOG.md)

---

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º

| –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å | Transformers | Transformers Forge |
|-------------|--------------|-------------------|
| EMA –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ | ‚ùå | ‚úÖ +1-3% |
| Layer freezing | –í—Ä—É—á–Ω—É—é | ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–æ |
| Training presets | ‚ùå | ‚úÖ SFT/LoRA/QLoRA/DPO |
| Bug fixes | –ñ–¥–∞—Ç—å PR | ‚úÖ –°—Ä–∞–∑—É |

---

## üìÅ –ù–æ–≤—ã–µ –º–æ–¥—É–ª–∏

```
src/transformers/
‚îú‚îÄ‚îÄ ema.py               # EMA –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
‚îú‚îÄ‚îÄ layer_utils.py       # –ó–∞–º–æ—Ä–æ–∑–∫–∞ —Å–ª–æ—ë–≤
‚îú‚îÄ‚îÄ training_presets.py  # –ì–æ—Ç–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏
‚îú‚îÄ‚îÄ training_monitor.py  # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è
```

---

## ü§ù –í–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç

–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é—Ç—Å—è:
- üêõ –°–æ–æ–±—â–µ–Ω–∏—è –æ –±–∞–≥–∞—Ö
- üí° –ù–æ–≤—ã–µ –∏–¥–µ–∏
- üîß Pull requests

–°–º. [CONTRIBUTING_RU.md](CONTRIBUTING_RU.md)

---

## üìú –õ–∏—Ü–µ–Ω–∑–∏—è

Apache License 2.0

---

## üë§ –ê–≤—Ç–æ—Ä

**–°–∞–º–∞–¥ –ê–±–¥—É–ª–∞–µ–≤ (–§–∏—Ç–∞–ª—Å)**
- üìß Email: usnul.noxil@gmail.com
- üí° –ê–≤—Ç–æ—Ä –∏–¥–µ–∏ –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ Transformers Forge

---

## üôè –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏

- [Hugging Face](https://huggingface.co/) ‚Äî –∑–∞ –±–∞–∑–æ–≤—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É Transformers
- Community ‚Äî –∑–∞ –∏–¥–µ–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

---

**üî® Transformers Forge v1.0.0 ‚Äî –∫—É—ë–º –ª—É—á—à–µ–µ!**

*Created by –°–∞–º–∞–¥ –ê–±–¥—É–ª–∞–µ–≤ (Fitals)*
