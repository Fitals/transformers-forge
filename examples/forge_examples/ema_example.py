"""
Transformers Forge: –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è EMA
=============================================

EMA (Exponential Moving Average) —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –Ω–∞ +1-3%.

–ó–∞–ø—É—Å–∫:
    python ema_example.py
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from transformers.ema import EMACallback, compute_optimal_decay, print_ema_info
from datasets import Dataset


def main():
    print("=" * 60)
    print("üî® Transformers Forge: EMA Example")
    print("=" * 60)
    
    # =========================================================================
    # 1. –î–µ–º–æ-–º–æ–¥–µ–ª—å (–º–∞–ª–µ–Ω—å–∫–∞—è –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞)
    # =========================================================================
    
    print("\nüì• Loading demo model...")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–ª–µ–Ω—å–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    model_name = "gpt2"
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    
    print(f"‚úÖ Model loaded: {model_name}")
    
    # =========================================================================
    # 2. –î–µ–º–æ-–¥–∞—Ç–∞—Å–µ—Ç
    # =========================================================================
    
    print("\nüìö Creating demo dataset...")
    
    texts = [
        "The quick brown fox jumps over the lazy dog.",
        "Machine learning is transforming the world.",
        "Transformers Forge makes training easier.",
        "EMA improves model quality by 1-3%.",
    ] * 100  # –ü–æ–≤—Ç–æ—Ä—è–µ–º –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    
    dataset = Dataset.from_dict({"text": texts})
    
    def tokenize(example):
        return tokenizer(
            example["text"],
            truncation=True,
            max_length=128,
            padding="max_length"
        )
    
    dataset = dataset.map(tokenize, batched=True)
    dataset.set_format(type="torch", columns=["input_ids", "attention_mask"])
    
    print(f"‚úÖ Dataset created: {len(dataset)} samples")
    
    # =========================================================================
    # 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ EMA
    # =========================================================================
    
    print("\n‚öôÔ∏è Configuring EMA...")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á—ë—Ç decay
    total_steps = len(dataset) // 4 * 3  # batch_size=4, epochs=3
    optimal_decay = compute_optimal_decay(total_steps)
    
    print(f"Total steps: {total_steps}")
    print(f"Optimal decay: {optimal_decay:.6f}")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ EMA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    print_ema_info(decay=0.999, total_steps=total_steps)
    
    # –°–æ–∑–¥–∞—ë–º callback
    ema_callback = EMACallback(decay=0.999)
    
    # =========================================================================
    # 4. –û–±—É—á–µ–Ω–∏–µ
    # =========================================================================
    
    print("\nüöÄ Starting training with EMA...")
    
    training_args = TrainingArguments(
        output_dir="./output_ema_example",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        logging_steps=50,
        save_strategy="no",
        report_to="none",
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        callbacks=[ema_callback]
    )
    
    trainer.train()
    
    # =========================================================================
    # 5. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ EMA –≤–µ—Å–æ–≤
    # =========================================================================
    
    print("\nüìä Applying EMA weights...")
    
    # –î–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è EMA
    print("Before EMA: Regular training weights")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º EMA
    original_weights = ema_callback.apply_ema(model)
    print("After EMA: Smoothed weights applied")
    
    # =========================================================================
    # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    # =========================================================================
    
    print("\nüíæ Saving model with EMA weights...")
    
    model.save_pretrained("./output_ema_example/model_with_ema")
    tokenizer.save_pretrained("./output_ema_example/model_with_ema")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º EMA state –æ—Ç–¥–µ–ª—å–Ω–æ
    ema_state = ema_callback.get_ema_state()
    torch.save(ema_state, "./output_ema_example/ema_state.pt")
    
    print("‚úÖ Model and EMA state saved!")
    
    # =========================================================================
    # 7. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ (–¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è)
    # =========================================================================
    
    print("\nüîÑ Demonstration: Restoring original weights...")
    ema_callback.restore_original(model)
    print("‚úÖ Original weights restored")
    
    print("\n" + "=" * 60)
    print("üéâ EMA Example completed!")
    print("=" * 60)
    print("\nKey takeaways:")
    print("  ‚Ä¢ EMA decay=0.999 is a good default for most training")
    print("  ‚Ä¢ Call apply_ema() BEFORE saving for best quality")
    print("  ‚Ä¢ EMA typically improves eval metrics by +1-3%")


if __name__ == "__main__":
    main()
