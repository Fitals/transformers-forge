"""
Transformers Forge: –ü—Ä–∏–º–µ—Ä –∑–∞–º–æ—Ä–æ–∑–∫–∏ —Å–ª–æ—ë–≤
==========================================

Layer freezing —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å –∏ –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ (LP-LoRA —Å—Ç–∏–ª—å).

–ó–∞–ø—É—Å–∫:
    python layer_freezing_example.py
"""

from transformers import AutoModelForCausalLM
from transformers import (
    freeze_first_n_layers,
    freeze_embeddings,
    unfreeze_model,
    get_num_layers,
    get_trainable_params,
    get_frozen_percentage,
    print_layer_status,
    setup_lp_lora_style,
    get_memory_savings_estimate,
    GradualUnfreezer
)


def main():
    print("=" * 60)
    print("üî® Transformers Forge: Layer Freezing Example")
    print("=" * 60)
    
    # =========================================================================
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    # =========================================================================
    
    print("\nüì• Loading model...")
    
    model_name = "gpt2"
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    num_layers = get_num_layers(model)
    total_params = get_trainable_params(model)
    
    print(f"‚úÖ Model: {model_name}")
    print(f"   Layers: {num_layers}")
    print(f"   Parameters: {total_params:,}")
    
    # =========================================================================
    # 2. –ë–∞–∑–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å
    # =========================================================================
    
    print("\nüìä Initial state:")
    print(f"   Trainable: {get_trainable_params(model):,}")
    print(f"   Frozen: {get_frozen_percentage(model):.1f}%")
    
    # =========================================================================
    # 3. –ó–∞–º–æ—Ä–æ–∑–∫–∞ –ø–µ—Ä–≤—ã—Ö N —Å–ª–æ—ë–≤
    # =========================================================================
    
    print("\nü•∂ Freezing first 6 layers...")
    
    freeze_first_n_layers(model, n=6)
    
    print(f"   Trainable: {get_trainable_params(model):,}")
    print(f"   Frozen: {get_frozen_percentage(model):.1f}%")
    
    # =========================================================================
    # 4. –î–µ—Ç–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å
    # =========================================================================
    
    print("\nüìã Layer status:")
    print_layer_status(model)
    
    # =========================================================================
    # 5. –†–∞–∑–º–æ—Ä–æ–∑–∫–∞
    # =========================================================================
    
    print("\nüî• Unfreezing all...")
    
    unfreeze_model(model)
    print(f"   Frozen: {get_frozen_percentage(model):.1f}%")
    
    # =========================================================================
    # 6. LP-LoRA —Å—Ç–∏–ª—å (50% –∑–∞–º–æ—Ä–æ–∑–∫–∞)
    # =========================================================================
    
    print("\n‚ö° Setting up LP-LoRA style (50% frozen)...")
    
    setup_lp_lora_style(model, freeze_ratio=0.5)
    
    print(f"   Trainable: {get_trainable_params(model):,}")
    print(f"   Frozen: {get_frozen_percentage(model):.1f}%")
    
    # =========================================================================
    # 7. –û—Ü–µ–Ω–∫–∞ —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    # =========================================================================
    
    print("\nüíæ Memory savings estimate:")
    
    savings = get_memory_savings_estimate(model)
    print(f"   Gradient memory saved: {savings['gradient_saved_gb']:.4f} GB")
    print(f"   Optimizer memory saved: {savings['optimizer_saved_gb']:.4f} GB")
    print(f"   Total saved: {savings['total_saved_gb']:.4f} GB")
    
    # =========================================================================
    # 8. –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∞ (–¥–µ–º–æ)
    # =========================================================================
    
    print("\nüîÑ Gradual unfreezing demonstration:")
    
    # –°–±—Ä–æ—Å
    unfreeze_model(model)
    
    # –°–æ–∑–¥–∞—ë–º unfreezer –¥–ª—è 5 —ç–ø–æ—Ö
    unfreezer = GradualUnfreezer(model, total_epochs=5, verbose=True)
    
    for epoch in range(5):
        print(f"\n   Epoch {epoch + 1}:")
        unfreezer.step(epoch)
        print(f"   Frozen: {get_frozen_percentage(model):.1f}%")
    
    # =========================================================================
    # –ò—Ç–æ–≥–∏
    # =========================================================================
    
    print("\n" + "=" * 60)
    print("üéâ Layer Freezing Example completed!")
    print("=" * 60)
    print("\nKey takeaways:")
    print("  ‚Ä¢ freeze_first_n_layers() –¥–ª—è LP-LoRA —Å—Ç–∏–ª—è")
    print("  ‚Ä¢ setup_lp_lora_style(model, 0.5) –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
    print("  ‚Ä¢ GradualUnfreezer –¥–ª—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–π —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∏")
    print("  ‚Ä¢ –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–∞ % –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö —Å–ª–æ—ë–≤")


if __name__ == "__main__":
    main()
