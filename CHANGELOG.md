# üî® Transformers Forge - Changelog

–í—Å–µ –∑–Ω–∞—á–∏–º—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —ç—Ç–æ–º –ø—Ä–æ–µ–∫—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É—é—Ç—Å—è –≤ —ç—Ç–æ–º —Ñ–∞–π–ª–µ.

–§–æ—Ä–º–∞—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ [Keep a Changelog](https://keepachangelog.com/ru/1.0.0/),
–∏ —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è [–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è](https://semver.org/lang/ru/).

---

## [1.0.2] - 2024-12-18 ‚Äî Tests Verified

### ‚úÖ –¢–µ—Å—Ç—ã

- **–ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã —Ç–µ—Å—Ç—ã** ‚Äî –≤—Å–µ unit —Ç–µ—Å—Ç—ã —Ç–µ–ø–µ—Ä—å –ø—Ä–æ—Ö–æ–¥—è—Ç (100%)
- `test_ema.py` ‚Äî –∏—Å–ø—Ä–∞–≤–ª–µ–Ω –ø–æ–¥ —Ä–µ–∞–ª—å–Ω—ã–π API
- `test_layer_utils.py` ‚Äî –≤—Å–µ —Ç–µ—Å—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç
- `test_training_presets.py` ‚Äî –∏—Å–ø—Ä–∞–≤–ª–µ–Ω –ø–æ–¥ —Ä–µ–∞–ª—å–Ω—ã–π API
- `test_training_monitor.py` ‚Äî –∏—Å–ø—Ä–∞–≤–ª–µ–Ω –ø–æ–¥ —Ä–µ–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ –ø–æ–ª–µ–π

### üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è

- –¢–µ—Å—Ç—ã —Ç–µ–ø–µ—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ä–µ–∞–ª—å–Ω—ã–º —Å–∏–≥–Ω–∞—Ç—É—Ä–∞–º —Ñ—É–Ω–∫—Ü–∏–π
- –£–±—Ä–∞–Ω—ã –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞—Ç—Ä–∏–±—É—Ç—ã –∏–∑ —Ç–µ—Å—Ç–æ–≤

---

## [1.0.1] - 2024-12-18 ‚Äî –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ –¢–µ—Å—Ç—ã

### üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- **docs/index.md** ‚Äî –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
- **docs/api/** ‚Äî API Reference –¥–ª—è –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π:
  - `ema.md` ‚Äî –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è EMA
  - `layer_utils.md` ‚Äî –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Layer Utils
  - `training_presets.md` ‚Äî –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Training Presets
  - `training_monitor.md` ‚Äî –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Training Monitor
- **docs/tutorials/** ‚Äî –ì–∞–π–¥—ã –∏ tutorials:
  - `quickstart.md` ‚Äî –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
  - `ema_guide.md` ‚Äî –ü–æ–¥—Ä–æ–±–Ω—ã–π –≥–∞–π–¥ –ø–æ EMA
  - `finetuning.md` ‚Äî –ü–æ–ª–Ω—ã–π –≥–∞–π–¥ –ø–æ fine-tuning

### üíª –ü—Ä–∏–º–µ—Ä—ã

- **examples/forge_examples/** ‚Äî –†–∞–±–æ—á–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞:
  - `ema_example.py` ‚Äî –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è EMA
  - `layer_freezing_example.py` ‚Äî –ü—Ä–∏–º–µ—Ä –∑–∞–º–æ—Ä–æ–∑–∫–∏ —Å–ª–æ—ë–≤
  - `training_presets_example.py` ‚Äî –ü—Ä–∏–º–µ—Ä Training Presets

### üß™ –¢–µ—Å—Ç—ã

- **tests/forge/** ‚Äî Unit —Ç–µ—Å—Ç—ã –¥–ª—è –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π:
  - `test_ema.py` ‚Äî –¢–µ—Å—Ç—ã EMA
  - `test_layer_utils.py` ‚Äî –¢–µ—Å—Ç—ã Layer Utils
  - `test_training_presets.py` ‚Äî –¢–µ—Å—Ç—ã Training Presets
  - `test_training_monitor.py` ‚Äî –¢–µ—Å—Ç—ã Training Monitor

### üèõÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

- **ROADMAP.md** ‚Äî –ü–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è –ø—Ä–æ–µ–∫—Ç–∞
- **GOVERNANCE.md** ‚Äî –ú–æ–¥–µ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
- **.github/ISSUE_TEMPLATE/** ‚Äî –®–∞–±–ª–æ–Ω—ã –¥–ª—è Issues:
  - `bug_report.md`
  - `feature_request.md`
  - `question.md`

---

## [1.0.0] - 2024-12-18 ‚Äî –ü–µ—Ä–≤—ã–π —Ä–µ–ª–∏–∑ Transformers Forge!

### ‚ú® –ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ (New Features)

#### Training Monitor Module
- **–§–∞–π–ª:** `src/transformers/training_monitor.py`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** –ù–æ–≤—ã–π –º–æ–¥—É–ª—å –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
- **–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
  - `count_parameters()` ‚Äî –ø–æ–¥—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (total/trainable/frozen)
  - `get_parameter_breakdown()` ‚Äî –¥–µ—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ —Å–ª–æ—è–º
  - `estimate_model_memory()` ‚Äî –æ—Ü–µ–Ω–∫–∞ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
  - `get_gpu_memory_info()` ‚Äî –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –ø–∞–º—è—Ç–∏
  - `check_gradient_health()` ‚Äî –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (NaN, Inf, vanishing, exploding)
  - `TrainingMonitor` ‚Äî –∫–ª–∞—Å—Å –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
  - `MonitorCallback` ‚Äî callback –¥–ª—è Trainer —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
  - `print_model_info()` / `print_gpu_status()` ‚Äî –±—ã—Å—Ç—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≤—ã–≤–æ–¥–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```python
from transformers.training_monitor import TrainingMonitor, MonitorCallback

# –ê–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏
monitor = TrainingMonitor(model)
monitor.print_model_summary()

# –° Trainer
trainer = Trainer(model=model, callbacks=[MonitorCallback()])
```

#### GitHub Actions CI/CD –¥–ª—è Enhanced –≤–µ—Ä—Å–∏–∏
- **–§–∞–π–ª—ã:** `.github/workflows/enhanced-*.yml`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** –ù–∞–±–æ—Ä –ª—ë–≥–∫–∏—Ö CI/CD workflows –¥–ª—è Enhanced –≤–µ—Ä—Å–∏–∏
- **Workflows:**
  - `enhanced-code-quality.yml` ‚Äî Lint (Ruff), –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤, –≤–µ—Ä—Å–∏–π, —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞
  - `enhanced-tests.yml` ‚Äî –¢–µ—Å—Ç—ã GenerationConfig, training_monitor, –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–∏–∫—Å–æ–≤
  - `enhanced-release.yml` ‚Äî –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–ª–∏–∑–∞, —Å–±–æ—Ä–∫–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
- **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
  - –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö GitHub-hosted runners (–±–µ–∑ GPU)
  - –ù–µ —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ–∫—Ä–µ—Ç–æ–≤ HuggingFace
  - –ë—ã—Å—Ç—Ä–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (~3-5 –º–∏–Ω—É—Ç)

#### Training Presets Module
- **–§–∞–π–ª:** `src/transformers/training_presets.py`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** –ì–æ—Ç–æ–≤—ã–µ, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM
- **Presets:**
  - `SFTPreset` ‚Äî Supervised Fine-Tuning —Å NEFTune
  - `LoRAPreset` ‚Äî LoRA fine-tuning (PEFT)
  - `QLoRAPreset` ‚Äî 4-bit Quantized LoRA  
  - `DPOPreset` ‚Äî Direct Preference Optimization
  - `MemoryEfficientPreset` ‚Äî –î–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π GPU –ø–∞–º—è—Ç–∏
- **–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
  - –ê–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ GPU/CPU –∏ bf16 –ø–æ–¥–¥–µ—Ä–∂–∫–∏
  - –ì–æ—Ç–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è PEFT (LoraConfig) –∏ BitsAndBytes
  - Registry –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö presets
  - Quick-—Ñ—É–Ω–∫—Ü–∏–∏: `quick_sft_args()`, `quick_lora_args()`

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```python
from transformers.training_presets import get_preset

# –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
preset = get_preset("lora", lora_r=16)
training_args = preset.get_training_args()
lora_config = preset.get_lora_config()
```

#### Layer Utilities Module
- **–§–∞–π–ª:** `src/transformers/layer_utils.py`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ—è–º–∏ (–∑–∞–º–æ—Ä–æ–∑–∫–∞/—Ä–∞–∑–º–æ—Ä–æ–∑–∫–∞)
- **–§—É–Ω–∫—Ü–∏–∏:**
  - `freeze_first_n_layers()` ‚Äî –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å –ø–µ—Ä–≤—ã–µ N —Å–ª–æ—ë–≤ (LP-LoRA —Å—Ç–∏–ª—å)
  - `freeze_except_last_n()` ‚Äî –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å –≤—Å—ë –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N
  - `freeze_embeddings()` ‚Äî –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
  - `get_trainable_params()` / `get_frozen_percentage()` ‚Äî –∞–Ω–∞–ª–∏–∑
  - `print_layer_status()` ‚Äî —Ç–∞–±–ª–∏—Ü–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å–ª–æ—ë–≤
  - `GradualUnfreezer` ‚Äî –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∞ –¥–ª—è transfer learning
  - `setup_lp_lora_style()` ‚Äî –±—ã—Å—Ç—Ä–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LP-LoRA
- **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
  - 100% –±–µ–∑–æ–ø–∞—Å–Ω–æ ‚Äî –Ω–µ –∏–∑–º–µ–Ω—è–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
  - –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –¥–æ 50%+ (–º–µ–Ω—å—à–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)
  - –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```python
from transformers import freeze_first_n_layers, get_frozen_percentage

freeze_first_n_layers(model, n=16)  # LP-LoRA —Å—Ç–∏–ª—å
print(f"Frozen: {get_frozen_percentage(model):.1f}%")
```

#### EMA (Exponential Moving Average) Module
- **–§–∞–π–ª:** `src/transformers/ema.py`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** –°–≥–ª–∞–∂–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –ª—É—á—à–µ–π generalization
- **–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**
  - `EMACallback` ‚Äî Callback –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Trainer
  - `EMAModel` ‚Äî –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è EMA
  - `compute_optimal_decay()` ‚Äî –†–∞—Å—á—ë—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ decay
- **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
  - **+1-3% –Ω–∞ eval –º–µ—Ç—Ä–∏–∫–∞—Ö** (—Ç–∏–ø–∏—á–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ)
  - –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
  - –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º–∏ (Polyak averaging, 1990+)
  - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ SOTA: Stable Diffusion, DALL-E

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```python
from transformers import Trainer
from transformers.ema import EMACallback

trainer = Trainer(
    model=model,
    args=args,
    callbacks=[EMACallback(decay=0.999)]
)
trainer.train()

# –ü—Ä–∏–º–µ–Ω–∏—Ç—å EMA –≤–µ—Å–∞ (–±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ)
ema_callback.apply_ema(model)
```

### üêõ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–∞–≥–æ–≤ (Bug Fixes)

#### Fix #1: TvpConfig –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç `type_vocab_size` (Issue #42925)
- **–§–∞–π–ª:** `src/transformers/models/tvp/configuration_tvp.py`
- **–ü—Ä–æ–±–ª–µ–º–∞:** `TvpConfig` –Ω–µ –∏–º–µ–ª –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ `type_vocab_size`, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏–ª–æ –∫ `AttributeError` –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º –∫–æ–Ω—Ñ–∏–≥–æ–º.
- **–†–µ—à–µ–Ω–∏–µ:** –î–æ–±–∞–≤–ª–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä `type_vocab_size=2` –≤ `__init__` –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é.

#### Fix #2: Qwen2VLImageProcessor –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä `size` (Issue #42910)
- **–§–∞–π–ª:** `src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py`
- **–ü—Ä–æ–±–ª–µ–º–∞:** –ü–∞—Ä–∞–º–µ—Ç—Ä `size` –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–ª—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –∏–∑-–∑–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –ª–æ–≥–∏–∫–∏ `if/else`.
- **–†–µ—à–µ–Ω–∏–µ:** –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –ª–æ–≥–∏–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ‚Äî —Ç–µ–ø–µ—Ä—å —è–≤–Ω–æ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π `size` —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è.

#### Fix #3: ConditionalDetr —Ç–µ—Ä—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –∫–ª–∞—Å—Å –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (Issue #42679)
- **–§–∞–π–ª—ã:** 
  - `src/transformers/models/conditional_detr/image_processing_conditional_detr.py`
  - `src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py`
- **–ü—Ä–æ–±–ª–µ–º–∞:** –ú–µ—Ç–æ–¥ `post_process_semantic_segmentation` –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —É–¥–∞–ª—è–ª –ø–æ—Å–ª–µ–¥–Ω–∏–π –∫–ª–∞—Å—Å (`[..., :-1]`), —á—Ç–æ –±—ã–ª–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ DETR (–∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç null class), –Ω–æ Conditional DETR –Ω–µ –∏–º–µ–µ—Ç null class.
- **–†–µ—à–µ–Ω–∏–µ:** –£–±—Ä–∞–Ω–æ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Å—Ä–µ–∑–∞–Ω–∏–µ, –≤—Å–µ –∫–ª–∞—Å—Å—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è.

#### Fix #4: OneFormerProcessor `task_inputs` –Ω–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ (Issue #42722)
- **–§–∞–π–ª:** `src/transformers/models/oneformer/processing_oneformer.py`
- **–ü—Ä–æ–±–ª–µ–º–∞:** `task_inputs` –æ—Å—Ç–∞–≤–∞–ª–∏—Å—å –Ω–∞ CPU –¥–∞–∂–µ –∫–æ–≥–¥–∞ `pixel_values` –±—ã–ª–∏ –Ω–∞ GPU, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏–ª–æ –∫ –æ—à–∏–±–∫–µ device mismatch.
- **–†–µ—à–µ–Ω–∏–µ:** –î–æ–±–∞–≤–ª–µ–Ω–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ `task_inputs` —Å `pixel_values`.

#### Fix #5: SAM HQ —Ç–µ—Å—Ç—ã –ø–∞–¥–∞—é—Ç –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è `set_seed()` (Issue #42890)
- **–§–∞–π–ª:** `tests/models/sam_hq/test_modeling_sam_hq.py`
- **–ü—Ä–æ–±–ª–µ–º–∞:** –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –Ω–µ –∏–º–µ–ª–∏ `set_seed()`, —á—Ç–æ –¥–µ–ª–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ–≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–º–∏.
- **–†–µ—à–µ–Ω–∏–µ:** –î–æ–±–∞–≤–ª–µ–Ω `setUp` –º–µ—Ç–æ–¥ —Å `set_seed(0)` –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏.

#### Fix #6: SiglipModel –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç `hidden_states` (Issue #42759)
- **–§–∞–π–ª:** `src/transformers/models/siglip/modeling_siglip.py`
- **–ü—Ä–æ–±–ª–µ–º–∞:** `SiglipTextTransformer` –Ω–µ –∏–º–µ–ª –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–∞ `@check_model_inputs`, –ø–æ—ç—Ç–æ–º—É `output_hidden_states=True` –Ω–µ —Ä–∞–±–æ—Ç–∞–ª.
- **–†–µ—à–µ–Ω–∏–µ:** –î–æ–±–∞–≤–ª–µ–Ω –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä `@check_model_inputs(tie_last_hidden_states=False)`.

#### Fix #7: GenerationConfig –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç —è–≤–Ω–æ –∑–∞–¥–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (Issue #42762)
- **–§–∞–π–ª—ã:** 
  - `src/transformers/generation/configuration_utils.py`
  - `src/transformers/generation/utils.py`
  - `tests/generation/test_configuration_utils.py`
- **–ü—Ä–æ–±–ª–µ–º–∞:** –õ–æ–≥–∏–∫–∞ —Å–ª–∏—è–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥–æ–≤ –Ω–µ –º–æ–≥–ª–∞ –æ—Ç–ª–∏—á–∏—Ç—å —è–≤–Ω–æ –∑–∞–¥–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç –¥–µ—Ñ–æ–ª—Ç–Ω—ã—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —è–≤–Ω–æ –∑–∞–¥–∞–ª `temperature=1.0`, –∞ –º–æ–¥–µ–ª—å –∏–º–µ–ª–∞ `temperature=1e-06`, –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–ª–æ—Å—å.
- **–†–µ—à–µ–Ω–∏–µ:** 
  - –î–æ–±–∞–≤–ª–µ–Ω –∞—Ç—Ä–∏–±—É—Ç `_explicitly_set_attrs` –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —è–≤–Ω–æ –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
  - –ò–∑–º–µ–Ω–µ–Ω–∞ –ª–æ–≥–∏–∫–∞ —Å–ª–∏—è–Ω–∏—è ‚Äî —è–≤–Ω–æ –∑–∞–¥–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ù–ï –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—é—Ç—Å—è.
  - –î–æ–±–∞–≤–ª–µ–Ω—ã —Ç–µ—Å—Ç—ã –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏.
- **–í–∞–∂–Ω–æ –¥–ª—è:** RLHF, DPO, GRPO, TRL training ‚Äî —Ç–µ–ø–µ—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è.

---

## –ö–∞–∫ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å

```bash
# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
git clone https://github.com/YOUR_USERNAME/transformers-enhanced.git
cd transformers-enhanced

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤ —Ä–µ–∂–∏–º–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
pip install -e .
```

---

## –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

- **–ë–∞–∑–æ–≤–∞—è –≤–µ—Ä—Å–∏—è:** transformers 5.0.0.dev0
- **Python:** 3.9+
- **PyTorch:** 2.0+

---

## –ê–≤—Ç–æ—Ä—ã

- Community Enhanced Version
- –û—Ä–∏–≥–∏–Ω–∞–ª: Hugging Face Team
